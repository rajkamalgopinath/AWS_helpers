{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3af79ae-4e7a-4353-b7e9-3717b51ee467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!git config --global user.name \"Chris Endemann\"\n",
    "!git config --global user.email endeman@wisc.edu\n",
    "github_url = 'github.com/UW-Madison-DataScience/test_AWS.git' # found under Code -> Clone -> HTTPS (remote the https:// before the rest of the address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf82450-c6b3-4ad6-9764-f0ed8a9d5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GitHub Username:  qualiaMachine\n",
      "GitHub Personal Access Token (PAT):  ········\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "\n",
    "# # Prompt for GitHub username and PAT securely\n",
    "# username = input(\"GitHub Username: \")\n",
    "# token = getpass.getpass(\"GitHub Personal Access Token (PAT): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1876e11-3e1d-4fe4-a236-932542fe1775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/test_AWS\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# !cd test_AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fdf8b1-3c6d-422a-bb95-a38c70c16673",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbdiff /tmp/git-blob-7tPqMh/05_Intro-train-models.ipynb 05_Intro-train-models.ipynb\n",
      "--- /tmp/git-blob-7tPqMh/05_Intro-train-models.ipynb  2024-11-01 23:01:32.936419\n",
      "+++ 05_Intro-train-models.ipynb  2024-11-01 23:00:25.168139\n",
      "\u001b[34m\u001b[1m## replaced /cells/1/execution_count:\u001b[0m\n",
      "\u001b[31m-  25\n",
      "\u001b[32m+  1\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/1/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "\u001b[32m+      sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-  26\n",
      "\u001b[32m+  2\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/5/execution_count:\u001b[0m\n",
      "\u001b[31m-  27\n",
      "\u001b[32m+  3\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/5/outputs/0/text:\u001b[0m\n",
      "\u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[31m-File downloaded: ./titanic_test.csv\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mFile downloaded: ./titanic_train.csv\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/7/execution_count:\u001b[0m\n",
      "\u001b[31m-  3\n",
      "\u001b[32m+  4\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/9/execution_count:\u001b[0m\n",
      "\u001b[31m-  28\n",
      "\u001b[32m+  5\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/11/execution_count:\u001b[0m\n",
      "\u001b[31m-  31\n",
      "\u001b[32m+  6\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/11/outputs/0/text:\u001b[0m\n",
      "\u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[31m-Training time: 0.06 seconds\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mTraining time: 0.07 seconds\u001b[m\n",
      " Model saved to ./xgboost-model\u001b[m\n",
      "\u001b[31m-Local training time: 0.10 seconds\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mLocal training time: 1.10 seconds\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/11/outputs/1:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stderr\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "\u001b[32m+      Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "\u001b[32m+        warnings.warn(\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/13/execution_count:\u001b[0m\n",
      "\u001b[31m-  34\n",
      "\u001b[32m+  7\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/13/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      An error occurred (UnauthorizedOperation) when calling the DescribeInstanceTypeOfferings operation: You are not authorized to perform this operation. User: arn:aws:sts::183295408236:assumed-role/ml-sagemaker-use/SageMaker is not authorized to perform: ec2:DescribeInstanceTypeOfferings because no identity-based policy allows the ec2:DescribeInstanceTypeOfferings action\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/15/execution_count:\u001b[0m\n",
      "\u001b[31m-  37\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/15/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stderr\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-58-45-158\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      2024-11-01 22:58:46 Starting - Starting the training job...\n",
      "\u001b[32m+      2024-11-01 22:59:01 Starting - Preparing the instances for training...\n",
      "\u001b[32m+      2024-11-01 22:59:25 Downloading - Downloading input data...\n",
      "\u001b[32m+      2024-11-01 23:00:05 Downloading - Downloading the training image.\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/15/outputs/0-1:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stderr\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "\u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.large.\n",
      "\u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-25-59-504\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      2024-11-01 22:26:01 Starting - Starting the training job...\n",
      "\u001b[31m-      2024-11-01 22:26:17 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      2024-11-01 22:26:41 Downloading - Downloading input data...\n",
      "\u001b[31m-      2024-11-01 22:27:21 Downloading - Downloading the training image......\n",
      "\u001b[31m-      2024-11-01 22:28:32 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      2024-11-01 22:28:32 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:28:22.062 ip-10-0-251-187.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:28:22.085 ip-10-0-251-187.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[31m-      \u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[31m-      \u001b[34mProcessing /opt/ml/code\n",
      "\u001b[31m-        Preparing metadata (setup.py): started\n",
      "\u001b[31m-        Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[31m-      \u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "\u001b[31m-        Building wheel for train-xgboost (setup.py): started\u001b[0m\n",
      "\u001b[31m-      \u001b[34m  Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "\u001b[31m-        Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=10623 sha256=b31dfae0c3499467efbbc51c882ab217c3c21cd87f27d0dcd8ec2d51b58b77de\n",
      "\u001b[31m-        Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-2l39uija/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[31m-      \u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:28:24:INFO] Invoking user script\u001b[0m\n",
      "\u001b[31m-      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[31m-      \u001b[34m{\n",
      "\u001b[31m-          \"additional_framework_parameters\": {},\n",
      "\u001b[31m-          \"channel_input_dirs\": {\n",
      "\u001b[31m-              \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[31m-          },\n",
      "\u001b[31m-          \"current_host\": \"algo-1\",\n",
      "\u001b[31m-          \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "\u001b[31m-          \"hosts\": [\n",
      "\u001b[31m-              \"algo-1\"\n",
      "\u001b[31m-          ],\n",
      "\u001b[31m-          \"hyperparameters\": {\n",
      "\u001b[31m-              \"colsample_bytree\": 0.8,\n",
      "\u001b[31m-              \"eta\": 0.1,\n",
      "\u001b[31m-              \"max_depth\": 5,\n",
      "\u001b[31m-              \"num_round\": 100,\n",
      "\u001b[31m-              \"subsample\": 0.8,\n",
      "\u001b[31m-              \"train\": \"titanic_train.csv\"\n",
      "\u001b[31m-          },\n",
      "\u001b[31m-          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[31m-          \"input_data_config\": {\n",
      "\u001b[31m-              \"train\": {\n",
      "\u001b[31m-                  \"ContentType\": \"csv\",\n",
      "\u001b[31m-                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[31m-                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[31m-                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[31m-              }\n",
      "\u001b[31m-          },\n",
      "\u001b[31m-          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[31m-          \"is_master\": true,\n",
      "\u001b[31m-          \"job_name\": \"sagemaker-xgboost-2024-11-01-22-25-59-504\",\n",
      "\u001b[31m-          \"log_level\": 20,\n",
      "\u001b[31m-          \"master_hostname\": \"algo-1\",\n",
      "\u001b[31m-          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[31m-          \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\n",
      "\u001b[31m-          \"module_name\": \"train_xgboost\",\n",
      "\u001b[31m-          \"network_interface_name\": \"eth0\",\n",
      "\u001b[31m-          \"num_cpus\": 2,\n",
      "\u001b[31m-          \"num_gpus\": 0,\n",
      "\u001b[31m-          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[31m-          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[31m-          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[31m-          \"resource_config\": {\n",
      "\u001b[31m-              \"current_host\": \"algo-1\",\n",
      "\u001b[31m-              \"current_instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-              \"hosts\": [\n",
      "\u001b[31m-                  \"algo-1\"\n",
      "\u001b[31m-              ],\n",
      "\u001b[31m-              \"instance_groups\": [\n",
      "\u001b[31m-                  {\n",
      "\u001b[31m-                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-                      \"instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-                      \"hosts\": [\n",
      "\u001b[31m-                          \"algo-1\"\n",
      "\u001b[31m-                      ]\n",
      "\u001b[31m-                  }\n",
      "\u001b[31m-              ],\n",
      "\u001b[31m-              \"network_interface_name\": \"eth0\"\n",
      "\u001b[31m-          },\n",
      "\u001b[31m-          \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[31m-      \u001b[34m}\u001b[0m\n",
      "\u001b[31m-      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-11-01-22-25-59-504\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\",\"--train\",\"titanic_train.csv\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[34mSM_HP_TRAIN=titanic_train.csv\u001b[0m\n",
      "\u001b[31m-      \u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[31m-      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8 --train titanic_train.csv\u001b[0m\n",
      "\u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[34mTraining time: 0.22 seconds\u001b[0m\n",
      "\u001b[31m-      \u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      2024-11-01 22:28:45 Completed - Training job completed\n",
      "\u001b[31m-      Training seconds: 125\n",
      "\u001b[31m-      Billable seconds: 125\n",
      "\u001b[31m-      Training time on SageMaker: 197.60 seconds\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/19/execution_count:\u001b[0m\n",
      "\u001b[31m-  38\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/19/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      xgboost/sagemaker-xgboost-2024-11-01-22-25-59-504/output/model.tar.gz\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/20/execution_count:\u001b[0m\n",
      "\u001b[31m-  40\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/21/execution_count:\u001b[0m\n",
      "\u001b[31m-  41\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/21/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: execute_result\n",
      "\u001b[31m-    execution_count: 41\n",
      "\u001b[31m-    data:\n",
      "\u001b[31m-      text/html:\n",
      "\u001b[31m-        <div>\n",
      "\u001b[31m-        <style scoped>\n",
      "\u001b[31m-            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[31m-                vertical-align: middle;\n",
      "\u001b[31m-            }\n",
      "\u001b[31m-        \n",
      "\u001b[31m-            .dataframe tbody tr th {\n",
      "\u001b[31m-                vertical-align: top;\n",
      "\u001b[31m-            }\n",
      "\u001b[31m-        \n",
      "\u001b[31m-            .dataframe thead th {\n",
      "\u001b[31m-                text-align: right;\n",
      "\u001b[31m-            }\n",
      "\u001b[31m-        </style>\n",
      "\u001b[31m-        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[31m-          <thead>\n",
      "\u001b[31m-            <tr style=\"text-align: right;\">\n",
      "\u001b[31m-              <th></th>\n",
      "\u001b[31m-              <th>PassengerId</th>\n",
      "\u001b[31m-              <th>Survived</th>\n",
      "\u001b[31m-              <th>Pclass</th>\n",
      "\u001b[31m-              <th>Name</th>\n",
      "\u001b[31m-              <th>Sex</th>\n",
      "\u001b[31m-              <th>Age</th>\n",
      "\u001b[31m-              <th>SibSp</th>\n",
      "\u001b[31m-              <th>Parch</th>\n",
      "\u001b[31m-              <th>Ticket</th>\n",
      "\u001b[31m-              <th>Fare</th>\n",
      "\u001b[31m-              <th>Cabin</th>\n",
      "\u001b[31m-              <th>Embarked</th>\n",
      "\u001b[31m-            </tr>\n",
      "\u001b[31m-          </thead>\n",
      "\u001b[31m-          <tbody>\n",
      "\u001b[31m-            <tr>\n",
      "\u001b[31m-              <th>0</th>\n",
      "\u001b[31m-              <td>566</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>3</td>\n",
      "\u001b[31m-              <td>Davies, Mr. Alfred J</td>\n",
      "\u001b[31m-              <td>male</td>\n",
      "\u001b[31m-              <td>24.0</td>\n",
      "\u001b[31m-              <td>2</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>A/4 48871</td>\n",
      "\u001b[31m-              <td>24.1500</td>\n",
      "\u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-              <td>S</td>\n",
      "\u001b[31m-            </tr>\n",
      "\u001b[31m-            <tr>\n",
      "\u001b[31m-              <th>1</th>\n",
      "\u001b[31m-              <td>161</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>3</td>\n",
      "\u001b[31m-              <td>Cribb, Mr. John Hatfield</td>\n",
      "\u001b[31m-              <td>male</td>\n",
      "\u001b[31m-              <td>44.0</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>1</td>\n",
      "\u001b[31m-              <td>371362</td>\n",
      "\u001b[31m-              <td>16.1000</td>\n",
      "\u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-              <td>S</td>\n",
      "\u001b[31m-            </tr>\n",
      "\u001b[31m-            <tr>\n",
      "\u001b[31m-              <th>2</th>\n",
      "\u001b[31m-              <td>554</td>\n",
      "\u001b[31m-              <td>1</td>\n",
      "\u001b[31m-              <td>3</td>\n",
      "\u001b[31m-              <td>Leeni, Mr. Fahim (\"Philip Zenni\")</td>\n",
      "\u001b[31m-              <td>male</td>\n",
      "\u001b[31m-              <td>22.0</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>2620</td>\n",
      "\u001b[31m-              <td>7.2250</td>\n",
      "\u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-              <td>C</td>\n",
      "\u001b[31m-            </tr>\n",
      "\u001b[31m-            <tr>\n",
      "\u001b[31m-              <th>3</th>\n",
      "\u001b[31m-              <td>861</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>3</td>\n",
      "\u001b[31m-              <td>Hansen, Mr. Claus Peter</td>\n",
      "\u001b[31m-              <td>male</td>\n",
      "\u001b[31m-              <td>41.0</td>\n",
      "\u001b[31m-              <td>2</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>350026</td>\n",
      "\u001b[31m-              <td>14.1083</td>\n",
      "\u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-              <td>S</td>\n",
      "\u001b[31m-            </tr>\n",
      "\u001b[31m-            <tr>\n",
      "\u001b[31m-              <th>4</th>\n",
      "\u001b[31m-              <td>242</td>\n",
      "\u001b[31m-              <td>1</td>\n",
      "\u001b[31m-              <td>3</td>\n",
      "\u001b[31m-              <td>Murphy, Miss. Katherine \"Kate\"</td>\n",
      "\u001b[31m-              <td>female</td>\n",
      "\u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-              <td>1</td>\n",
      "\u001b[31m-              <td>0</td>\n",
      "\u001b[31m-              <td>367230</td>\n",
      "\u001b[31m-              <td>15.5000</td>\n",
      "\u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-              <td>Q</td>\n",
      "\u001b[31m-            </tr>\n",
      "\u001b[31m-          </tbody>\n",
      "\u001b[31m-        </table>\n",
      "\u001b[31m-        </div>\n",
      "\u001b[31m-      text/plain:\n",
      "\u001b[31m-           PassengerId  Survived  Pclass                               Name     Sex  \\\n",
      "\u001b[31m-        0          566         0       3               Davies, Mr. Alfred J    male   \n",
      "\u001b[31m-        1          161         0       3           Cribb, Mr. John Hatfield    male   \n",
      "\u001b[31m-        2          554         1       3  Leeni, Mr. Fahim (\"Philip Zenni\")    male   \n",
      "\u001b[31m-        3          861         0       3            Hansen, Mr. Claus Peter    male   \n",
      "\u001b[31m-        4          242         1       3     Murphy, Miss. Katherine \"Kate\"  female   \n",
      "\u001b[31m-        \n",
      "\u001b[31m-            Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
      "\u001b[31m-        0  24.0      2      0  A/4 48871  24.1500   NaN        S  \n",
      "\u001b[31m-        1  44.0      0      1     371362  16.1000   NaN        S  \n",
      "\u001b[31m-        2  22.0      0      0       2620   7.2250   NaN        C  \n",
      "\u001b[31m-        3  41.0      2      0     350026  14.1083   NaN        S  \n",
      "\u001b[31m-        4   NaN      1      0     367230  15.5000   NaN        Q  \n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/22/execution_count:\u001b[0m\n",
      "\u001b[31m-  43\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/23/execution_count:\u001b[0m\n",
      "\u001b[31m-  44\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/23/outputs/0-1:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      Test Set Accuracy: 0.7933\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stderr\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "\u001b[31m-      configuration generated by an older version of XGBoost, please export the model by calling\n",
      "\u001b[31m-      `Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\u001b[31m-      \n",
      "\u001b[31m-          https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      for more details about differences between saving model and serializing.\n",
      "\u001b[31m-      \n",
      "\u001b[31m-        warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "\u001b[31m-        warnings.warn(smsg, UserWarning)\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/26/execution_count:\u001b[0m\n",
      "\u001b[31m-  48\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/26/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: execute_result\n",
      "\u001b[31m-    execution_count: 48\n",
      "\u001b[31m-    data:\n",
      "\u001b[31m-      text/plain: 's3://titanic-dataset-test/data/titanic_train.csv'\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/27/execution_count:\u001b[0m\n",
      "\u001b[31m-  49\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/27/outputs/0-2:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stderr\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "\u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-48-56-105\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      2024-11-01 22:48:57 Starting - Starting the training job...\n",
      "\u001b[31m-      2024-11-01 22:49:11 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      2024-11-01 22:49:42 Downloading - Downloading input data...\n",
      "\u001b[31m-      2024-11-01 22:50:22 Downloading - Downloading the training image......\n",
      "\u001b[31m-      2024-11-01 22:51:29 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      2024-11-01 22:51:29 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:23.778 ip-10-2-247-41.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:23.803 ip-10-2-247-41.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Single node training.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Train matrix has 713 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:24.213 ip-10-2-247-41.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:24.214 ip-10-2-247-41.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[0]#011train-rmse:474.71030\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:24.227 ip-10-2-247-41.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-11-01 22:51:24.229 ip-10-2-247-41.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[1]#011train-rmse:441.03842\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2]#011train-rmse:411.78134\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[3]#011train-rmse:385.35440\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[4]#011train-rmse:362.34192\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[5]#011train-rmse:342.72199\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[6]#011train-rmse:325.37424\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[7]#011train-rmse:310.20413\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[8]#011train-rmse:297.79462\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[9]#011train-rmse:287.85199\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[10]#011train-rmse:277.92941\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[11]#011train-rmse:270.85162\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[12]#011train-rmse:263.09851\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[13]#011train-rmse:257.25269\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[14]#011train-rmse:251.85989\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[15]#011train-rmse:247.19409\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[16]#011train-rmse:243.73045\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[17]#011train-rmse:240.81642\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[18]#011train-rmse:238.41530\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[19]#011train-rmse:235.56351\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[20]#011train-rmse:233.57898\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[21]#011train-rmse:231.39540\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[22]#011train-rmse:228.63503\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[23]#011train-rmse:226.69484\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[24]#011train-rmse:225.35779\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[25]#011train-rmse:223.92523\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[26]#011train-rmse:222.10831\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[27]#011train-rmse:219.23029\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[28]#011train-rmse:218.87340\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[29]#011train-rmse:216.75085\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[30]#011train-rmse:215.76749\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[31]#011train-rmse:214.97679\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[32]#011train-rmse:213.81511\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[33]#011train-rmse:212.42398\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[34]#011train-rmse:211.10745\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[35]#011train-rmse:209.56615\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[36]#011train-rmse:208.38251\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[37]#011train-rmse:207.96460\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[38]#011train-rmse:206.41853\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[39]#011train-rmse:205.13840\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[40]#011train-rmse:204.59671\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[41]#011train-rmse:203.43626\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[42]#011train-rmse:202.23776\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[43]#011train-rmse:201.98227\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[44]#011train-rmse:201.53015\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[45]#011train-rmse:200.83151\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[46]#011train-rmse:199.75769\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[47]#011train-rmse:197.73955\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[48]#011train-rmse:196.67972\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[49]#011train-rmse:195.99304\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[50]#011train-rmse:194.60979\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[51]#011train-rmse:193.87764\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[52]#011train-rmse:193.04419\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[53]#011train-rmse:191.84062\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[54]#011train-rmse:191.63332\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[55]#011train-rmse:191.06137\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[56]#011train-rmse:190.63503\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[57]#011train-rmse:190.23791\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[58]#011train-rmse:190.01700\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[59]#011train-rmse:189.62627\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[60]#011train-rmse:188.78932\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[61]#011train-rmse:187.87903\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[62]#011train-rmse:187.33061\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[63]#011train-rmse:186.93269\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[64]#011train-rmse:186.04112\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[65]#011train-rmse:185.29774\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[66]#011train-rmse:184.67114\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[67]#011train-rmse:183.74358\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[68]#011train-rmse:183.30225\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[69]#011train-rmse:182.09914\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[70]#011train-rmse:181.83897\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[71]#011train-rmse:181.03862\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[72]#011train-rmse:180.78651\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[73]#011train-rmse:179.64867\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[74]#011train-rmse:178.82935\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[75]#011train-rmse:178.21071\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[76]#011train-rmse:177.54585\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[77]#011train-rmse:177.00539\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[78]#011train-rmse:176.26054\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[79]#011train-rmse:175.64746\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[80]#011train-rmse:174.62911\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[81]#011train-rmse:174.01623\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[82]#011train-rmse:173.50301\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[83]#011train-rmse:172.43010\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[84]#011train-rmse:171.95624\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[85]#011train-rmse:171.48639\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[86]#011train-rmse:171.19154\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[87]#011train-rmse:169.97925\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[88]#011train-rmse:169.45494\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[89]#011train-rmse:168.90468\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[90]#011train-rmse:168.16402\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[91]#011train-rmse:167.30739\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[92]#011train-rmse:166.85228\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[93]#011train-rmse:165.98686\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[94]#011train-rmse:165.70697\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[95]#011train-rmse:165.43739\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[96]#011train-rmse:164.83107\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[97]#011train-rmse:164.22020\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[98]#011train-rmse:163.90085\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[99]#011train-rmse:163.45399\u001b[0m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      2024-11-01 22:51:43 Completed - Training job completed\n",
      "\u001b[31m-      Training seconds: 120\n",
      "\u001b[31m-      Billable seconds: 120\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: error\n",
      "\u001b[31m-    ename: NameError\n",
      "\u001b[31m-    evalue: name 'instance_type' is not defined\n",
      "\u001b[31m-    traceback:\n",
      "\u001b[31m-      item[0]: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m-      item[1]: \u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[31m-      item[2]:\n",
      "\u001b[31m-        Cell \u001b[0;32mIn[49], line 28\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m     25\u001b[0m xgboost_estimator_builtin\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_input})\n",
      "\u001b[31m-        \u001b[1;32m     26\u001b[0m end \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[31m-        \u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime for training on SageMaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, instance_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43minstance_type\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, instance_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[31m-      item[3]: \u001b[0;31mNameError\u001b[0m: name 'instance_type' is not defined\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/31/execution_count:\u001b[0m\n",
      "\u001b[31m-  11\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/31/outputs/0-3:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stderr\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "\u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-19-57-38-692\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      2024-10-29 19:57:39 Starting - Starting the training job...\n",
      "\u001b[31m-      2024-10-29 19:57:54 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      2024-10-29 19:58:27 Downloading - Downloading input data......\n",
      "\u001b[31m-      2024-10-29 19:59:18 Downloading - Downloading the training image...\n",
      "\u001b[31m-      2024-10-29 20:00:09 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:14.589 ip-10-0-150-29.ec2.internal:8 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:14.614 ip-10-0-150-29.ec2.internal:8 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Single node training.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:15.170 ip-10-0-150-29.ec2.internal:8 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:15.171 ip-10-0-150-29.ec2.internal:8 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:15.172 ip-10-0-150-29.ec2.internal:8 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:15.172 ip-10-0-150-29.ec2.internal:8 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[0]#011train-rmse:475.71411\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:15.232 ip-10-0-150-29.ec2.internal:8 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:00:15.234 ip-10-0-150-29.ec2.internal:8 INFO hook.py:491] Hook is writing from the hook with pid: 8\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[1]#011train-rmse:441.49982\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2]#011train-rmse:411.72672\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[3]#011train-rmse:385.56036\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[4]#011train-rmse:363.56360\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[5]#011train-rmse:344.49588\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[6]#011train-rmse:327.67941\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[7]#011train-rmse:312.88376\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[8]#011train-rmse:299.41186\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[9]#011train-rmse:288.74734\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[10]#011train-rmse:280.12149\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[11]#011train-rmse:273.00226\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[12]#011train-rmse:266.27246\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[13]#011train-rmse:259.31256\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[14]#011train-rmse:253.75191\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[15]#011train-rmse:249.62512\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[16]#011train-rmse:245.67200\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[17]#011train-rmse:242.34294\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[18]#011train-rmse:238.86392\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[19]#011train-rmse:235.89893\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[20]#011train-rmse:233.08173\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[21]#011train-rmse:231.52962\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[22]#011train-rmse:229.66519\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[23]#011train-rmse:227.86470\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[24]#011train-rmse:226.72954\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[25]#011train-rmse:225.20438\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[26]#011train-rmse:223.76180\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[27]#011train-rmse:221.96107\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[28]#011train-rmse:221.09845\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[29]#011train-rmse:219.99710\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[30]#011train-rmse:219.37936\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[31]#011train-rmse:218.05364\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[32]#011train-rmse:217.43600\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[33]#011train-rmse:216.73910\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[34]#011train-rmse:216.43459\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[35]#011train-rmse:215.56277\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[36]#011train-rmse:214.80632\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[37]#011train-rmse:213.70375\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[38]#011train-rmse:213.17102\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[39]#011train-rmse:212.88145\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[40]#011train-rmse:211.96532\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[41]#011train-rmse:211.32878\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[42]#011train-rmse:210.17601\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[43]#011train-rmse:209.69156\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[44]#011train-rmse:208.88245\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[45]#011train-rmse:207.83882\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[46]#011train-rmse:206.71755\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[47]#011train-rmse:205.46107\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[48]#011train-rmse:204.46623\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[49]#011train-rmse:203.54263\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[50]#011train-rmse:202.88849\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[51]#011train-rmse:202.33600\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[52]#011train-rmse:201.32494\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[53]#011train-rmse:200.73129\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[54]#011train-rmse:200.46822\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[55]#011train-rmse:199.81789\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[56]#011train-rmse:199.07161\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[57]#011train-rmse:198.43318\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[58]#011train-rmse:198.14304\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[59]#011train-rmse:197.53601\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[60]#011train-rmse:197.10297\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[61]#011train-rmse:196.49066\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[62]#011train-rmse:196.28507\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[63]#011train-rmse:195.67941\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[64]#011train-rmse:195.51599\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[65]#011train-rmse:194.97147\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[66]#011train-rmse:193.90741\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[67]#011train-rmse:193.52390\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[68]#011train-rmse:193.01285\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[69]#011train-rmse:192.34790\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[70]#011train-rmse:191.98561\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[71]#011train-rmse:191.39389\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[72]#011train-rmse:190.95151\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[73]#011train-rmse:190.21582\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[74]#011train-rmse:189.08704\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[75]#011train-rmse:188.47955\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[76]#011train-rmse:188.12349\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[77]#011train-rmse:187.77058\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[78]#011train-rmse:187.10945\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[79]#011train-rmse:186.61465\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[80]#011train-rmse:185.80434\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[81]#011train-rmse:184.99844\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[82]#011train-rmse:184.62537\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[83]#011train-rmse:184.16344\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[84]#011train-rmse:183.58179\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[85]#011train-rmse:183.19162\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[86]#011train-rmse:182.80438\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[87]#011train-rmse:182.35306\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[88]#011train-rmse:181.88933\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[89]#011train-rmse:181.32742\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[90]#011train-rmse:180.80669\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[91]#011train-rmse:180.49007\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[92]#011train-rmse:179.89072\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[93]#011train-rmse:179.37184\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[94]#011train-rmse:179.06938\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[95]#011train-rmse:178.65883\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[96]#011train-rmse:177.99016\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[97]#011train-rmse:177.48030\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[98]#011train-rmse:177.15761\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[99]#011train-rmse:176.45931\u001b[0m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      2024-10-29 20:00:32 Uploading - Uploading generated training model\n",
      "\u001b[31m-      2024-10-29 20:00:32 Completed - Training job completed\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stderr\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-20-00-56-211\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      Training seconds: 125\n",
      "\u001b[31m-      Billable seconds: 125\n",
      "\u001b[31m-      Training time with 1 instance: 197.52 seconds\n",
      "\u001b[31m-      2024-10-29 20:00:56 Starting - Starting the training job...\n",
      "\u001b[31m-      2024-10-29 20:01:24 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      2024-10-29 20:01:57 Downloading - Downloading input data......\n",
      "\u001b[31m-      2024-10-29 20:02:47 Downloading - Downloading the training image......\n",
      "\u001b[31m-      2024-10-29 20:04:01 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      2024-10-29 20:04:01 Uploading - Uploading generated training model\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:44.521 ip-10-2-73-243.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:44.558 ip-10-2-73-243.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Distributed node training with 2 hosts: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[20:03:45] task NULL got new rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:44.237 ip-10-2-108-166.ec2.internal:6 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:44.269 ip-10-2-108-166.ec2.internal:6 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Distributed node training with 2 hosts: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] start listen on algo-1:9099\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9099}\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] No data received from connection ('10.2.108.166', 47654). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] No data received from connection ('10.2.73.243', 55330). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[20:03:45] task NULL got new rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Recieve start signal from 10.2.108.166; assign rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Recieve start signal from 10.2.73.243; assign rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] @tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] @tracker 0.003575563430786133 secs between node start and job finish\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] start listen on algo-1:9100\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9100}\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] No data received from connection ('10.2.108.166', 59764). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] No data received from connection ('10.2.73.243', 33634). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[20:03:48] task NULL got new rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Recieve start signal from 10.2.108.166; assign rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Recieve start signal from 10.2.73.243; assign rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] @tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:48.590 ip-10-2-108-166.ec2.internal:6 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:48.591 ip-10-2-108-166.ec2.internal:6 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:48.592 ip-10-2-108-166.ec2.internal:6 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:48.592 ip-10-2-108-166.ec2.internal:6 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:48:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[20:03:48] task NULL got new rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:48:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:48.591 ip-10-2-73-243.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:48.591 ip-10-2-73-243.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:48.592 ip-10-2-73-243.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:48.592 ip-10-2-73-243.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29:20:03:48:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[35m[2024-10-29 20:03:48.651 ip-10-2-73-243.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29 20:03:48.650 ip-10-2-108-166.ec2.internal:6 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [0]#011train-rmse:475.06763\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [1]#011train-rmse:441.48178\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [2]#011train-rmse:411.14572\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [3]#011train-rmse:384.41522\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [4]#011train-rmse:362.00281\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [5]#011train-rmse:342.09195\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [6]#011train-rmse:325.18576\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [7]#011train-rmse:310.38946\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [8]#011train-rmse:296.75525\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [9]#011train-rmse:285.56943\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [10]#011train-rmse:276.39905\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [11]#011train-rmse:267.94705\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [12]#011train-rmse:261.27862\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [13]#011train-rmse:255.99045\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [14]#011train-rmse:251.24669\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [15]#011train-rmse:246.39867\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [16]#011train-rmse:242.99580\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [17]#011train-rmse:239.83401\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [18]#011train-rmse:237.54298\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [19]#011train-rmse:234.17052\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [20]#011train-rmse:231.99132\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [21]#011train-rmse:230.39975\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [22]#011train-rmse:228.85759\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [23]#011train-rmse:227.50473\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [24]#011train-rmse:225.93337\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [25]#011train-rmse:224.10906\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [26]#011train-rmse:222.46446\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [27]#011train-rmse:221.07739\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [28]#011train-rmse:219.62199\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [29]#011train-rmse:218.51106\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [30]#011train-rmse:217.05801\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [31]#011train-rmse:216.02483\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [32]#011train-rmse:215.17191\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [33]#011train-rmse:214.42673\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [34]#011train-rmse:213.53833\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [35]#011train-rmse:212.43672\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [36]#011train-rmse:211.95911\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [37]#011train-rmse:210.74970\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [38]#011train-rmse:209.58318\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [39]#011train-rmse:208.81479\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [40]#011train-rmse:207.43625\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [41]#011train-rmse:206.39166\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [42]#011train-rmse:205.92508\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [43]#011train-rmse:204.72928\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [44]#011train-rmse:204.11678\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [45]#011train-rmse:203.84581\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [46]#011train-rmse:202.89059\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [47]#011train-rmse:202.05829\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [48]#011train-rmse:201.17131\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [49]#011train-rmse:200.90657\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [50]#011train-rmse:199.62822\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [51]#011train-rmse:199.26662\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [52]#011train-rmse:198.40724\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [53]#011train-rmse:197.83804\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [54]#011train-rmse:197.20424\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [55]#011train-rmse:196.22171\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [56]#011train-rmse:195.52446\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [57]#011train-rmse:195.11131\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [58]#011train-rmse:194.62470\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [59]#011train-rmse:194.30669\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [60]#011train-rmse:193.58981\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [61]#011train-rmse:193.16104\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [62]#011train-rmse:192.49373\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [63]#011train-rmse:191.79701\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [64]#011train-rmse:191.18817\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [65]#011train-rmse:190.50592\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [66]#011train-rmse:190.10690\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [67]#011train-rmse:189.61542\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [68]#011train-rmse:188.87097\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [69]#011train-rmse:187.95456\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [70]#011train-rmse:187.44124\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [71]#011train-rmse:187.02232\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [72]#011train-rmse:186.86244\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [73]#011train-rmse:186.10358\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [74]#011train-rmse:185.34093\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [75]#011train-rmse:185.01708\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [76]#011train-rmse:184.52373\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [77]#011train-rmse:183.98309\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [78]#011train-rmse:183.27400\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [79]#011train-rmse:182.74863\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [80]#011train-rmse:181.90372\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [81]#011train-rmse:181.33293\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [82]#011train-rmse:180.66556\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [83]#011train-rmse:180.19649\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [84]#011train-rmse:179.81682\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [85]#011train-rmse:178.86356\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [86]#011train-rmse:178.28250\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [87]#011train-rmse:177.91344\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [88]#011train-rmse:177.26918\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [89]#011train-rmse:176.40164\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [90]#011train-rmse:175.97617\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [91]#011train-rmse:175.74719\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [92]#011train-rmse:175.13661\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [93]#011train-rmse:174.35800\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [94]#011train-rmse:173.84450\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [95]#011train-rmse:173.09387\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [96]#011train-rmse:172.90584\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [97]#011train-rmse:172.24793\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [98]#011train-rmse:171.71202\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [99]#011train-rmse:170.94412\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] @tracker 5.666167259216309 secs between node start and job finish\u001b[0m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      2024-10-29 20:04:15 Completed - Training job completed\n",
      "\u001b[31m-      Training seconds: 274\n",
      "\u001b[31m-      Billable seconds: 274\n",
      "\u001b[31m-      Training time with 2 instances: 228.55 seconds\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/33/execution_count:\u001b[0m\n",
      "\u001b[31m-  51\n",
      "\u001b[32m+  None\n",
      "\n",
      "\u001b[0mnbdiff /tmp/git-blob-U6ecbm/update-repo.ipynb update-repo.ipynb\n",
      "--- /tmp/git-blob-U6ecbm/update-repo.ipynb  2024-11-01 23:01:33.476421\n",
      "+++ update-repo.ipynb  2024-11-01 23:00:41.224206\n",
      "\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/1/execution_count:\u001b[0m\n",
      "\u001b[31m-  None\n",
      "\u001b[32m+  2\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## added /cells/1/metadata/tags:\u001b[0m\n",
      "\u001b[32m+  []\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/1/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdin\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      GitHub Username:  qualiaMachine\n",
      "\u001b[32m+      GitHub Personal Access Token (PAT):  ········\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/1/source:\u001b[0m\n",
      "\u001b[36m@@ -1,6 +1,6 @@\u001b[m\n",
      "\u001b[31m-import getpass\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m# import getpass\u001b[m\n",
      " \u001b[m\n",
      "\u001b[31m-# Prompt for GitHub username and PAT securely\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m# # Prompt for GitHub username and PAT securely\u001b[m\n",
      " # github_url = 'github.com/UW-Madison-DataScience/test_AWS.git' # found under Code -> Clone -> HTTPS (remote the https:// before the rest of the address)\u001b[m\n",
      " # username = input(\"GitHub Username: \")\u001b[m\n",
      " # token = getpass.getpass(\"GitHub Personal Access Token (PAT): \")\u001b[m\n",
      "\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/2/execution_count:\u001b[0m\n",
      "\u001b[31m-  None\n",
      "\u001b[32m+  3\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## added /cells/2/metadata/tags:\u001b[0m\n",
      "\u001b[32m+  []\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/2/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      /home/ec2-user/SageMaker/test_AWS\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-  None\n",
      "\u001b[32m+  4\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## added /cells/3/metadata/scrolled:\u001b[0m\n",
      "\u001b[32m+  True\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## added /cells/3/metadata/tags:\u001b[0m\n",
      "\u001b[32m+  []\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/3/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      nbdiff /tmp/git-blob-3vTqzX/03_Data-storage-and-access-via-buckets.ipynb 03_Data-storage-and-access-via-buckets.ipynb\n",
      "\u001b[32m+      --- /tmp/git-blob-3vTqzX/03_Data-storage-and-access-via-buckets.ipynb  2024-11-01 23:00:13.192088\n",
      "\u001b[32m+      +++ 03_Data-storage-and-access-via-buckets.ipynb  2024-11-01 22:01:28.321055\n",
      "\u001b[32m+      \u001b[34m\u001b[1m## modified /cells/2/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,5 +1,5 @@\u001b[m\n",
      "\u001b[32m+       ## 1B. Download copy into notebook environment\u001b[m\n",
      "\u001b[32m+      \u001b[31m-If you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local copy.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIf you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local\" copy (i.e., one stored in your notebook's instance).\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       Download data from S3 to notebook environment. You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/4-5:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  markdown cell:\n",
      "\u001b[32m+      \u001b[31m-    id: 61165195-70f5-45c0-aaad-ffb975ebc6ef\n",
      "\u001b[32m+      \u001b[31m-    source:\n",
      "\u001b[32m+      \u001b[31m-      ## 2. Pushing new files from notebook environment to bucket\n",
      "\u001b[32m+      \u001b[31m-      As your analysis generates new files, you can upload to your bucket as demonstrated below. For this demo, you can create a blank `results.txt` file to upload to your bucket.\n",
      "\u001b[32m+      \u001b[31m-  code cell:\n",
      "\u001b[32m+      \u001b[31m-    id: 2974d63f-cf39-4ff6-8544-ac8aa6be5e24\n",
      "\u001b[32m+      \u001b[31m-    execution_count: 14\n",
      "\u001b[32m+      \u001b[31m-    source:\n",
      "\u001b[32m+      \u001b[31m-      import boto3\n",
      "\u001b[32m+      \u001b[31m-      \n",
      "\u001b[32m+      \u001b[31m-      # Define the S3 bucket name and the file paths\n",
      "\u001b[32m+      \u001b[31m-      bucket_name = \"titanic-dataset-test\"\n",
      "\u001b[32m+      \u001b[31m-      train_file_path = \"results.txt\"\n",
      "\u001b[32m+      \u001b[31m-      \n",
      "\u001b[32m+      \u001b[31m-      # Initialize the S3 client\n",
      "\u001b[32m+      \u001b[31m-      s3 = boto3.client('s3')\n",
      "\u001b[32m+      \u001b[31m-      \n",
      "\u001b[32m+      \u001b[31m-      # Upload the training file\n",
      "\u001b[32m+      \u001b[31m-      s3.upload_file(train_file_path, bucket_name, \"results/results.txt\")\n",
      "\u001b[32m+      \u001b[31m-      \n",
      "\u001b[32m+      \u001b[31m-      print(\"Files uploaded successfully.\")\n",
      "\u001b[32m+      \u001b[31m-    outputs:\n",
      "\u001b[32m+      \u001b[31m-      output 0:\n",
      "\u001b[32m+      \u001b[31m-        output_type: stream\n",
      "\u001b[32m+      \u001b[31m-        name: stdout\n",
      "\u001b[32m+      \u001b[31m-        text:\n",
      "\u001b[32m+      \u001b[31m-          Files uploaded successfully.\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/6/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  ## 3. Check current size and storage costs of bucket\n",
      "\u001b[32m+      \u001b[32m+  ## 2. Check current size and storage costs of bucket\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/10/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,4 +1,4 @@\u001b[m\n",
      "\u001b[32m+      \u001b[31m-## 4: Check storage costs of bucket\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m## 3: Check storage costs of bucket\u001b[m\n",
      "\u001b[32m+       To estimate the storage cost of your Amazon S3 bucket directly from a Jupyter notebook in SageMaker, you can use the following approach. This method calculates the total size of the bucket and estimates the monthly storage cost based on AWS S3 pricing.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       **Note**: AWS S3 pricing varies by region and storage class. The example below uses the S3 Standard storage class pricing for the US East (N. Virginia) region as of November 1, 2024. Please verify the current pricing for your specific region and storage class on the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/13:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[32m+      \u001b[32m+    id: f60b6951-6549-41de-9948-0ee9832df0c1\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      ## 4. Pushing new files from notebook environment to bucket\n",
      "\u001b[32m+      \u001b[32m+      As your analysis generates new files, you can upload to your bucket as demonstrated below. For this demo, you can create a blank `results.txt` file to upload to your bucket.\n",
      "\u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 2974d63f-cf39-4ff6-8544-ac8aa6be5e24\n",
      "\u001b[32m+      \u001b[32m+    execution_count: 14\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      import boto3\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      # Define the S3 bucket name and the file paths\n",
      "\u001b[32m+      \u001b[32m+      bucket_name = \"titanic-dataset-test\"\n",
      "\u001b[32m+      \u001b[32m+      train_file_path = \"results.txt\"\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      # Initialize the S3 client\n",
      "\u001b[32m+      \u001b[32m+      s3 = boto3.client('s3')\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      # Upload the training file\n",
      "\u001b[32m+      \u001b[32m+      s3.upload_file(train_file_path, bucket_name, \"results/results.txt\")\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      print(\"Files uploaded successfully.\")\n",
      "\u001b[32m+      \u001b[32m+    outputs:\n",
      "\u001b[32m+      \u001b[32m+      output 0:\n",
      "\u001b[32m+      \u001b[32m+        output_type: stream\n",
      "\u001b[32m+      \u001b[32m+        name: stdout\n",
      "\u001b[32m+      \u001b[32m+        text:\n",
      "\u001b[32m+      \u001b[32m+          Files uploaded successfully.\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0mnbdiff /tmp/git-blob-d32cgw/04_Interacting-with-code-repo.ipynb 04_Interacting-with-code-repo.ipynb\n",
      "\u001b[32m+      --- /tmp/git-blob-d32cgw/04_Interacting-with-code-repo.ipynb  2024-11-01 23:00:13.696090\n",
      "\u001b[32m+      +++ 04_Interacting-with-code-repo.ipynb  2024-11-01 21:56:44.111790\n",
      "\u001b[32m+      \u001b[34m\u001b[1m## replaced /cells/20/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  88\n",
      "\u001b[32m+      \u001b[32m+  89\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/20/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[32m+      \u001b[32m+      [main 8102f2c] Updates from Jupyter notebooks\n",
      "\u001b[32m+      \u001b[32m+       8 files changed, 622 insertions(+), 311 deletions(-)\n",
      "\u001b[32m+      \u001b[32m+       create mode 100644 prep-train-test-sets.ipynb\n",
      "\u001b[32m+      \u001b[32m+       rename train_nn.py => scripts/train_nn.py (100%)\n",
      "\u001b[32m+      \u001b[32m+       rename train_xgboost.py => scripts/train_xgboost.py (100%)\n",
      "\u001b[32m+      \u001b[32m+       create mode 100644 update-repo.ipynb\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/20/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      [main c49ab7e] Updates from Jupyter notebooks\n",
      "\u001b[32m+      \u001b[31m-       1 file changed, 120 insertions(+), 569 deletions(-)\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/22/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  84\n",
      "\u001b[32m+      \u001b[32m+  90\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/22/outputs/0/text:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,4 +1,4 @@\u001b[m\n",
      "\u001b[32m+       From https://github.com/UW-Madison-DataScience/test_AWS\u001b[m\n",
      "\u001b[32m+        * branch            main       -> FETCH_HEAD\u001b[m\n",
      "\u001b[32m+      \u001b[31m-   0363cc2..bc28ce1  main       -> origin/main\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m   bc28ce1..22d83d0  main       -> origin/main\u001b[m\n",
      "\u001b[32m+       Already up to date.\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/24/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  52\n",
      "\u001b[32m+      \u001b[32m+  91\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/26/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  85\n",
      "\u001b[32m+      \u001b[32m+  92\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/26/outputs/0/text:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,9 +1,9 @@\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Enumerating objects: 5, done.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Counting objects: 100% (5/5), done.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mEnumerating objects: 17, done.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mCounting objects: 100% (17/17), done.\u001b[m\n",
      "\u001b[32m+       Delta compression using up to 2 threads\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Compressing objects: 100% (3/3), done.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Writing objects: 100% (3/3), 702 bytes | 702.00 KiB/s, done.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[m\n",
      "\u001b[32m+      \u001b[31m-remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mCompressing objects: 100% (12/12), done.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mWriting objects: 100% (12/12), 9.71 KiB | 2.43 MiB/s, done.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mTotal 12 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mremote: Resolving deltas: 100% (8/8), completed with 4 local objects.\u001b[K\u001b[m\n",
      "\u001b[32m+       To https://github.com/UW-Madison-DataScience/test_AWS.git\u001b[m\n",
      "\u001b[32m+      \u001b[31m-   bc28ce1..22d83d0  main -> main\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m   22d83d0..8102f2c  main -> main\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0mnbdiff /tmp/git-blob-dxpbq6/05_Intro-train-models.ipynb 05_Intro-train-models.ipynb\n",
      "\u001b[32m+      --- /tmp/git-blob-dxpbq6/05_Intro-train-models.ipynb  2024-11-01 23:00:14.236093\n",
      "\u001b[32m+      +++ 05_Intro-train-models.ipynb  2024-11-01 22:58:24.955638\n",
      "\u001b[32m+      \u001b[34m\u001b[1m## inserted before /cells/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[32m+      \u001b[32m+    id: bef72914-4f7a-41b8-a84f-15a0e438dd59\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      ## Initialize SageMaker environment\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      This code initializes the AWS SageMaker environment by defining the SageMaker role, session, and S3 client. It also specifies the S3 bucket and key for accessing the Titanic training dataset stored in an S3 bucket.\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  markdown cell:\n",
      "\u001b[32m+      \u001b[31m-    id: 46863ddc-86cf-4295-95bb-2c12e24a25a1\n",
      "\u001b[32m+      \u001b[31m-    source:\n",
      "\u001b[32m+      \u001b[31m-      ### Read data from S3 into memory\n",
      "\u001b[32m+      \u001b[31m-      Our data is stored on an S3 bucket called 'titanic-dataset-test'. We can use the following code to read data directly from S3 into memory in the Jupyter notebook environment, without actually downloading a copy of train.csv as a local file.\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/1/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  21\n",
      "\u001b[32m+      \u001b[32m+  25\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/1/outputs/0-1:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      (712, 12)\n",
      "\u001b[32m+      \u001b[31m-      (179, 12)\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: execute_result\n",
      "\u001b[32m+      \u001b[31m-    execution_count: 21\n",
      "\u001b[32m+      \u001b[31m-    data:\n",
      "\u001b[32m+      \u001b[31m-      text/html:\n",
      "\u001b[32m+      \u001b[31m-        <div>\n",
      "\u001b[32m+      \u001b[31m-        <style scoped>\n",
      "\u001b[32m+      \u001b[31m-            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[32m+      \u001b[31m-                vertical-align: middle;\n",
      "\u001b[32m+      \u001b[31m-            }\n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-            .dataframe tbody tr th {\n",
      "\u001b[32m+      \u001b[31m-                vertical-align: top;\n",
      "\u001b[32m+      \u001b[31m-            }\n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-            .dataframe thead th {\n",
      "\u001b[32m+      \u001b[31m-                text-align: right;\n",
      "\u001b[32m+      \u001b[31m-            }\n",
      "\u001b[32m+      \u001b[31m-        </style>\n",
      "\u001b[32m+      \u001b[31m-        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[32m+      \u001b[31m-          <thead>\n",
      "\u001b[32m+      \u001b[31m-            <tr style=\"text-align: right;\">\n",
      "\u001b[32m+      \u001b[31m-              <th></th>\n",
      "\u001b[32m+      \u001b[31m-              <th>PassengerId</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Survived</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Pclass</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Name</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Sex</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Age</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>SibSp</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Parch</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Ticket</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Fare</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Cabin</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Embarked</th>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-          </thead>\n",
      "\u001b[32m+      \u001b[31m-          <tbody>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>0</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>693</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Lam, Mr. Ali</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1601</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>56.4958</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>1</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>482</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>2</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Frost, Mr. Anthony Wood \"Archie\"</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>239854</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0.0000</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>2</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>528</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Farthing, Mr. John</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>PC 17483</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>221.7792</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>C95</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>3</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>856</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Aks, Mrs. Sam (Leah Rosen)</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>18.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>392091</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>9.3500</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>4</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>802</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>2</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Collyer, Mrs. Harvey (Charlotte Annie Tate)</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>31.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>C.A. 31921</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>26.2500</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-          </tbody>\n",
      "\u001b[32m+      \u001b[31m-        </table>\n",
      "\u001b[32m+      \u001b[31m-        </div>\n",
      "\u001b[32m+      \u001b[31m-      text/plain:\n",
      "\u001b[32m+      \u001b[31m-           PassengerId  Survived  Pclass                                         Name  \\\n",
      "\u001b[32m+      \u001b[31m-        0          693         1       3                                 Lam, Mr. Ali   \n",
      "\u001b[32m+      \u001b[31m-        1          482         0       2             Frost, Mr. Anthony Wood \"Archie\"   \n",
      "\u001b[32m+      \u001b[31m-        2          528         0       1                           Farthing, Mr. John   \n",
      "\u001b[32m+      \u001b[31m-        3          856         1       3                   Aks, Mrs. Sam (Leah Rosen)   \n",
      "\u001b[32m+      \u001b[31m-        4          802         1       2  Collyer, Mrs. Harvey (Charlotte Annie Tate)   \n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-              Sex   Age  SibSp  Parch      Ticket      Fare Cabin Embarked  \n",
      "\u001b[32m+      \u001b[31m-        0    male   NaN      0      0        1601   56.4958   NaN        S  \n",
      "\u001b[32m+      \u001b[31m-        1    male   NaN      0      0      239854    0.0000   NaN        S  \n",
      "\u001b[32m+      \u001b[31m-        2    male   NaN      0      0    PC 17483  221.7792   C95        S  \n",
      "\u001b[32m+      \u001b[31m-        3  female  18.0      0      1      392091    9.3500   NaN        S  \n",
      "\u001b[32m+      \u001b[31m-        4  female  31.0      1      1  C.A. 31921   26.2500   NaN        S  \n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/1/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -9,24 +9,9 @@\u001b[m \u001b[mrole = sagemaker.get_execution_role()\u001b[m\n",
      "\u001b[32m+       session = sagemaker.Session()\u001b[m\n",
      "\u001b[32m+       s3 = boto3.client('s3')\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-# Define the S3 bucket and object key\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define the S3 bucket\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[32m+       bucket = 'titanic-dataset-test'  # replace with your S3 bucket name\u001b[m\n",
      "\u001b[32m+      \u001b[31m-key = 'data/titanic_train.csv'  # replace with your object key\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-# Read the object from S3\u001b[m\n",
      "\u001b[32m+      \u001b[31m-response = s3.get_object(Bucket=bucket, Key=key)\u001b[m\n",
      "\u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[32m+      \u001b[31m-# Load the data into a pandas DataFrame\u001b[m\n",
      "\u001b[32m+      \u001b[31m-train_data = pd.read_csv(response['Body'])\u001b[m\n",
      "\u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[32m+      \u001b[31m-key = 'data/titanic_test.csv'  # replace with your object key\u001b[m\n",
      "\u001b[32m+      \u001b[31m-response = s3.get_object(Bucket=bucket, Key=key)\u001b[m\n",
      "\u001b[32m+      \u001b[31m-test_data = pd.read_csv(response['Body'])\u001b[m\n",
      "\u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[32m+      \u001b[31m-# check shape\u001b[m\n",
      "\u001b[32m+      \u001b[31m-print(train_data.shape)\u001b[m\n",
      "\u001b[32m+      \u001b[31m-print(test_data.shape)\u001b[m\n",
      "\u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[32m+      \u001b[31m-# Inspect the first few rows of the DataFrame\u001b[m\n",
      "\u001b[32m+      \u001b[31m-train_data.head()\u001b[m\n",
      "\u001b[32m+      \u001b[31m-# train_data.shape\u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define train/test filenames\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtrain_filename = 'titanic_train.csv'\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtest_filename = 'titanic_test.csv'\u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/2/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,5 +1,5 @@\u001b[m\n",
      "\u001b[32m+       ### Download copy into notebook environment\u001b[m\n",
      "\u001b[32m+      \u001b[31m-If you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local copy.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIf you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local\" copy (i.e., one that you store in your notebook's instance).\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       Download data from S3 to notebook environment. You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/3/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  22\n",
      "\u001b[32m+      \u001b[32m+  26\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/3/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,6 +1,6 @@\u001b[m\n",
      "\u001b[32m+       # Define the S3 bucket and file location\u001b[m\n",
      "\u001b[32m+      \u001b[31m-file_key = \"data/titanic_train.csv\"  # Path to your file in the S3 bucket\u001b[m\n",
      "\u001b[32m+      \u001b[31m-local_file_path = \"./titanic_train.csv\"  # Local path to save the file\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mfile_key = f\"data/{train_filename}\"  # Path to your file in the S3 bucket\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mlocal_file_path = f\"./{train_filename}\"  # Local path to save the file\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Initialize the S3 client and download the file\u001b[m\n",
      "\u001b[32m+       s3 = boto3.client(\"s3\")\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/4:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[32m+      \u001b[32m+    id: cd136750-a3be-409e-92d8-5430f70d68cf\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      We can do the same for the test set.\n",
      "\u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 253f4930-ca36-49ea-a2eb-fbb29826a394\n",
      "\u001b[32m+      \u001b[32m+    execution_count: 27\n",
      "\u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[32m+      \u001b[32m+        []\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      # Define the S3 bucket and file location\n",
      "\u001b[32m+      \u001b[32m+      file_key = f\"data/{train_filename}\"  # Path to your file in the S3 bucket. W\n",
      "\u001b[32m+      \u001b[32m+      local_file_path = f\"./{train_filename}\"  # Local path to save the file\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      # Initialize the S3 client and download the file\n",
      "\u001b[32m+      \u001b[32m+      s3 = boto3.client(\"s3\")\n",
      "\u001b[32m+      \u001b[32m+      s3.download_file(bucket, file_key, local_file_path)\n",
      "\u001b[32m+      \u001b[32m+      print(\"File downloaded:\", local_file_path)\n",
      "\u001b[32m+      \u001b[32m+    outputs:\n",
      "\u001b[32m+      \u001b[32m+      output 0:\n",
      "\u001b[32m+      \u001b[32m+        output_type: stream\n",
      "\u001b[32m+      \u001b[32m+        name: stdout\n",
      "\u001b[32m+      \u001b[32m+        text:\n",
      "\u001b[32m+      \u001b[32m+          File downloaded: ./titanic_test.csv\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/4/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,2 +1,2 @@\u001b[m\n",
      "\u001b[32m+       ### Get code (train and tune scripts) from git repo. \u001b[m\n",
      "\u001b[32m+      \u001b[31m-DO NOT put data inside your code repo, as version tracking for data files takes up unnecessary storage in this notebook instance.\u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mWe recommend you DO NOT put data inside your code repo, as version tracking for data files takes up unnecessary storage in this notebook instance. Instead, store your data in a separte S3 bucket. We have a data folder in our repo only as a means to initially hand you the data for this tutorial.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/6/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,2 +1,4 @@\u001b[m\n",
      "\u001b[32m+       ### Testing train.py on this notebook's instance\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mNotebook instances in SageMaker allow us allocate more powerful instances (or many instances) to machine learning jobs that require extra power, GPUs, or benefit from parallelization. Before we try exploiting this extra power, it is essential that we test our code thoroughly. We don't want to waste unnecessary compute cycles and resources on jobs that produce bugs instead of insights. If you need to, you can use a subset of your data to run quicker tests. You can also select a slightly better instance resource if your current instance insn't meeting your needs. See the [Instances for ML spreadsheet](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing) for guidance.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+       Test train.py on this notebook's instance (or when possible, on your own machine) before doing anything more complicated (e.g., hyperparameter tuning on multiple instances)\u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/7/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  4\n",
      "\u001b[32m+      \u001b[32m+  28\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/7/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[32m+      \u001b[32m+      Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.1.2)\n",
      "\u001b[32m+      \u001b[32m+      Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "\u001b[32m+      \u001b[32m+      Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.14.1)\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/7/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      Collecting xgboost\n",
      "\u001b[32m+      \u001b[31m-        Downloading xgboost-2.1.2-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "\u001b[32m+      \u001b[31m-      Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "\u001b[32m+      \u001b[31m-      Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.14.1)\n",
      "\u001b[32m+      \u001b[31m-      Downloading xgboost-2.1.2-py3-none-manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[32m+      \u001b[31m-      \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[?25hInstalling collected packages: xgboost\n",
      "\u001b[32m+      \u001b[31m-      Successfully installed xgboost-2.1.2\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/9/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  5\n",
      "\u001b[32m+      \u001b[32m+  31\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/9/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stderr\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "\u001b[32m+      \u001b[31m-      Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "\u001b[32m+      \u001b[31m-        warnings.warn(\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/9/outputs/1/text:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Training time: 0.08 seconds\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mTraining time: 0.06 seconds\u001b[m\n",
      "\u001b[32m+       Model saved to ./xgboost-model\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Local training time: 8.43 seconds\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mLocal training time: 0.10 seconds\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/9/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -3,7 +3,7 @@\u001b[m \u001b[mimport time as t\u001b[m\n",
      "\u001b[32m+       start_time = t.time()\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Run the script and pass arguments directly\u001b[m\n",
      "\u001b[32m+      \u001b[31m-%run test_AWS/train_xgboost.py --max_depth 5 --eta 0.1 --subsample 0.8 --colsample_bytree 0.8 --num_round 100 --train ./train.csv\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m%run test_AWS/scripts/train_xgboost.py --max_depth 5 --eta 0.1 --subsample 0.8 --colsample_bytree 0.8 --num_round 100 --train ./titanic_train.csv\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Measure and print the time taken\u001b[m\n",
      "\u001b[32m+       print(f\"Local training time: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/11/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  6\n",
      "\u001b[32m+      \u001b[32m+  34\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/11/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  !aws ec2 describe-instance-type-offerings --location-type us-east-2 --filters Name=instance-type,Values=ml.t3.medium\n",
      "\u001b[32m+      \u001b[32m+  # !aws ec2 describe-instance-type-offerings --location-type us-east-1 \n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/12/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -72,6 +72,8 @@\u001b[m \u001b[msklearn_estimator = SKLearn(\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       This setup simplifies training, allowing you to maintain custom environments directly within SageMaker’s managed containers, without needing to build and manage your own Docker images.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m---\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+       ### More information on pre-built environments\u001b[m\n",
      "\u001b[32m+       he [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) provides lists of pre-built container images for each framework and their standard libraries, including details on pre-installed packages.\u001b[m\n",
      "\u001b[32m+             \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/13/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  9\n",
      "\u001b[32m+      \u001b[32m+  37\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/13/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[32m+      \u001b[32m+    name: stderr\n",
      "\u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[32m+      \u001b[32m+      INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "\u001b[32m+      \u001b[32m+      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.large.\n",
      "\u001b[32m+      \u001b[32m+      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-25-59-504\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:26:01 Starting - Starting the training job...\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:26:17 Starting - Preparing the instances for training...\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:26:41 Downloading - Downloading input data...\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:27:21 Downloading - Downloading the training image......\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:28:32 Training - Training image download completed. Training in progress.\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:28:32 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[32m+        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:28:22.062 ip-10-0-251-187.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:28:22.085 ip-10-0-251-187.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mProcessing /opt/ml/code\n",
      "\u001b[32m+      \u001b[32m+        Preparing metadata (setup.py): started\n",
      "\u001b[32m+      \u001b[32m+        Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "\u001b[32m+      \u001b[32m+        Building wheel for train-xgboost (setup.py): started\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m  Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "\u001b[32m+      \u001b[32m+        Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=10623 sha256=b31dfae0c3499467efbbc51c882ab217c3c21cd87f27d0dcd8ec2d51b58b77de\n",
      "\u001b[32m+      \u001b[32m+        Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-2l39uija/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:24:INFO] Invoking user script\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m{\n",
      "\u001b[32m+      \u001b[32m+          \"additional_framework_parameters\": {},\n",
      "\u001b[32m+      \u001b[32m+          \"channel_input_dirs\": {\n",
      "\u001b[32m+      \u001b[32m+              \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[32m+      \u001b[32m+          },\n",
      "\u001b[32m+      \u001b[32m+          \"current_host\": \"algo-1\",\n",
      "\u001b[32m+      \u001b[32m+          \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "\u001b[32m+      \u001b[32m+          \"hosts\": [\n",
      "\u001b[32m+      \u001b[32m+              \"algo-1\"\n",
      "\u001b[32m+      \u001b[32m+          ],\n",
      "\u001b[32m+      \u001b[32m+          \"hyperparameters\": {\n",
      "\u001b[32m+      \u001b[32m+              \"colsample_bytree\": 0.8,\n",
      "\u001b[32m+      \u001b[32m+              \"eta\": 0.1,\n",
      "\u001b[32m+      \u001b[32m+              \"max_depth\": 5,\n",
      "\u001b[32m+      \u001b[32m+              \"num_round\": 100,\n",
      "\u001b[32m+      \u001b[32m+              \"subsample\": 0.8,\n",
      "\u001b[32m+      \u001b[32m+              \"train\": \"titanic_train.csv\"\n",
      "\u001b[32m+      \u001b[32m+          },\n",
      "\u001b[32m+      \u001b[32m+          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[32m+      \u001b[32m+          \"input_data_config\": {\n",
      "\u001b[32m+      \u001b[32m+              \"train\": {\n",
      "\u001b[32m+      \u001b[32m+                  \"ContentType\": \"csv\",\n",
      "\u001b[32m+      \u001b[32m+                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[32m+      \u001b[32m+                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[32m+      \u001b[32m+                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[32m+      \u001b[32m+              }\n",
      "\u001b[32m+      \u001b[32m+          },\n",
      "\u001b[32m+      \u001b[32m+          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[32m+      \u001b[32m+          \"is_master\": true,\n",
      "\u001b[32m+      \u001b[32m+          \"job_name\": \"sagemaker-xgboost-2024-11-01-22-25-59-504\",\n",
      "\u001b[32m+      \u001b[32m+          \"log_level\": 20,\n",
      "\u001b[32m+      \u001b[32m+          \"master_hostname\": \"algo-1\",\n",
      "\u001b[32m+      \u001b[32m+          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[32m+      \u001b[32m+          \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\n",
      "\u001b[32m+      \u001b[32m+          \"module_name\": \"train_xgboost\",\n",
      "\u001b[32m+      \u001b[32m+          \"network_interface_name\": \"eth0\",\n",
      "\u001b[32m+      \u001b[32m+          \"num_cpus\": 2,\n",
      "\u001b[32m+      \u001b[32m+          \"num_gpus\": 0,\n",
      "\u001b[32m+      \u001b[32m+          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[32m+      \u001b[32m+          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[32m+      \u001b[32m+          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[32m+      \u001b[32m+          \"resource_config\": {\n",
      "\u001b[32m+      \u001b[32m+              \"current_host\": \"algo-1\",\n",
      "\u001b[32m+      \u001b[32m+              \"current_instance_type\": \"ml.m5.large\",\n",
      "\u001b[32m+      \u001b[32m+              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+      \u001b[32m+              \"hosts\": [\n",
      "\u001b[32m+      \u001b[32m+                  \"algo-1\"\n",
      "\u001b[32m+      \u001b[32m+              ],\n",
      "\u001b[32m+      \u001b[32m+              \"instance_groups\": [\n",
      "\u001b[32m+      \u001b[32m+                  {\n",
      "\u001b[32m+      \u001b[32m+                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+      \u001b[32m+                      \"instance_type\": \"ml.m5.large\",\n",
      "\u001b[32m+      \u001b[32m+                      \"hosts\": [\n",
      "\u001b[32m+      \u001b[32m+                          \"algo-1\"\n",
      "\u001b[32m+      \u001b[32m+                      ]\n",
      "\u001b[32m+      \u001b[32m+                  }\n",
      "\u001b[32m+      \u001b[32m+              ],\n",
      "\u001b[32m+      \u001b[32m+              \"network_interface_name\": \"eth0\"\n",
      "\u001b[32m+      \u001b[32m+          },\n",
      "\u001b[32m+      \u001b[32m+          \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m}\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-11-01-22-25-59-504\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\",\"--train\",\"titanic_train.csv\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_TRAIN=titanic_train.csv\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8 --train titanic_train.csv\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[32m+        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[32m+        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[32m+        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mTraining time: 0.22 seconds\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:28:45 Completed - Training job completed\n",
      "\u001b[32m+      \u001b[32m+      Training seconds: 125\n",
      "\u001b[32m+      \u001b[32m+      Billable seconds: 125\n",
      "\u001b[32m+      \u001b[32m+      Training time on SageMaker: 197.60 seconds\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/13/outputs/0-1:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stderr\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-21-48-04-392\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:48:04 Starting - Starting the training job...\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:48:28 Starting - Preparing the instances for training...\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:48:59 Downloading - Downloading input data...\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:49:39 Downloading - Downloading the training image......\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:50:35 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 21:50:40.864 ip-10-2-116-141.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 21:50:40.894 ip-10-2-116-141.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mProcessing /opt/ml/code\n",
      "\u001b[32m+      \u001b[31m-        Preparing metadata (setup.py): started\n",
      "\u001b[32m+      \u001b[31m-        Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "\u001b[32m+      \u001b[31m-        Building wheel for train-xgboost (setup.py): started\n",
      "\u001b[32m+      \u001b[31m-        Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "\u001b[32m+      \u001b[31m-        Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=37357 sha256=508327b0c137859acefb3ed0abd6a872018edba9b29b73a748efb4bcf8f7b21d\n",
      "\u001b[32m+      \u001b[31m-        Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0glp7pi6/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:43:INFO] Invoking user script\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m{\n",
      "\u001b[32m+      \u001b[31m-          \"additional_framework_parameters\": {},\n",
      "\u001b[32m+      \u001b[31m-          \"channel_input_dirs\": {\n",
      "\u001b[32m+      \u001b[31m-              \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[32m+      \u001b[31m-          },\n",
      "\u001b[32m+      \u001b[31m-          \"current_host\": \"algo-1\",\n",
      "\u001b[32m+      \u001b[31m-          \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "\u001b[32m+      \u001b[31m-          \"hosts\": [\n",
      "\u001b[32m+      \u001b[31m-              \"algo-1\"\n",
      "\u001b[32m+      \u001b[31m-          ],\n",
      "\u001b[32m+      \u001b[31m-          \"hyperparameters\": {\n",
      "\u001b[32m+      \u001b[31m-              \"colsample_bytree\": 0.8,\n",
      "\u001b[32m+      \u001b[31m-              \"eta\": 0.1,\n",
      "\u001b[32m+      \u001b[31m-              \"max_depth\": 5,\n",
      "\u001b[32m+      \u001b[31m-              \"num_round\": 100,\n",
      "\u001b[32m+      \u001b[31m-              \"subsample\": 0.8\n",
      "\u001b[32m+      \u001b[31m-          },\n",
      "\u001b[32m+      \u001b[31m-          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[32m+      \u001b[31m-          \"input_data_config\": {\n",
      "\u001b[32m+      \u001b[31m-              \"train\": {\n",
      "\u001b[32m+      \u001b[31m-                  \"ContentType\": \"csv\",\n",
      "\u001b[32m+      \u001b[31m-                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[32m+      \u001b[31m-                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[32m+      \u001b[31m-                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[32m+      \u001b[31m-              }\n",
      "\u001b[32m+      \u001b[31m-          },\n",
      "\u001b[32m+      \u001b[31m-          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[32m+      \u001b[31m-          \"is_master\": true,\n",
      "\u001b[32m+      \u001b[31m-          \"job_name\": \"sagemaker-xgboost-2024-10-29-21-48-04-392\",\n",
      "\u001b[32m+      \u001b[31m-          \"log_level\": 20,\n",
      "\u001b[32m+      \u001b[31m-          \"master_hostname\": \"algo-1\",\n",
      "\u001b[32m+      \u001b[31m-          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[32m+      \u001b[31m-          \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-10-29-21-48-04-392/source/sourcedir.tar.gz\",\n",
      "\u001b[32m+      \u001b[31m-          \"module_name\": \"train_xgboost\",\n",
      "\u001b[32m+      \u001b[31m-          \"network_interface_name\": \"eth0\",\n",
      "\u001b[32m+      \u001b[31m-          \"num_cpus\": 2,\n",
      "\u001b[32m+      \u001b[31m-          \"num_gpus\": 0,\n",
      "\u001b[32m+      \u001b[31m-          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[32m+      \u001b[31m-          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[32m+      \u001b[31m-          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[32m+      \u001b[31m-          \"resource_config\": {\n",
      "\u001b[32m+      \u001b[31m-              \"current_host\": \"algo-1\",\n",
      "\u001b[32m+      \u001b[31m-              \"current_instance_type\": \"ml.m5.large\",\n",
      "\u001b[32m+      \u001b[31m-              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+      \u001b[31m-              \"hosts\": [\n",
      "\u001b[32m+      \u001b[31m-                  \"algo-1\"\n",
      "\u001b[32m+      \u001b[31m-              ],\n",
      "\u001b[32m+      \u001b[31m-              \"instance_groups\": [\n",
      "\u001b[32m+      \u001b[31m-                  {\n",
      "\u001b[32m+      \u001b[31m-                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+      \u001b[31m-                      \"instance_type\": \"ml.m5.large\",\n",
      "\u001b[32m+      \u001b[31m-                      \"hosts\": [\n",
      "\u001b[32m+      \u001b[31m-                          \"algo-1\"\n",
      "\u001b[32m+      \u001b[31m-                      ]\n",
      "\u001b[32m+      \u001b[31m-                  }\n",
      "\u001b[32m+      \u001b[31m-              ],\n",
      "\u001b[32m+      \u001b[31m-              \"network_interface_name\": \"eth0\"\n",
      "\u001b[32m+      \u001b[31m-          },\n",
      "\u001b[32m+      \u001b[31m-          \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m}\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8}\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-10-29-21-48-04-392/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-10-29-21-48-04-392\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-10-29-21-48-04-392/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mTraining time: 0.24 seconds\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:51:04 Uploading - Uploading generated training model\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 21:51:04 Completed - Training job completed\n",
      "\u001b[32m+      \u001b[31m-      Training seconds: 126\n",
      "\u001b[32m+      \u001b[31m-      Billable seconds: 126\n",
      "\u001b[32m+      \u001b[31m-      Training time on SageMaker: 197.74 seconds\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/13/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,9 +1,12 @@\u001b[m\n",
      "\u001b[32m+       from sagemaker.inputs import TrainingInput\u001b[m\n",
      "\u001b[32m+       from sagemaker.xgboost.estimator import XGBoost\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define instance type/count we'll use for training\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Define S3 paths for input and output\u001b[m\n",
      "\u001b[32m+      \u001b[31m-train_s3_path = f's3://{bucket}/train.csv'\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtrain_s3_path = f's3://{bucket}/data/{train_filename}'\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # we'll store all results in a subfolder called xgboost on our bucket. This folder will automatically be created if it doesn't exist already.\u001b[m\n",
      "\u001b[32m+       output_folder = 'xgboost'\u001b[m\n",
      "\u001b[32m+      \u001b[36m@@ -12,14 +15,15 @@\u001b[m \u001b[moutput_path = f's3://{bucket}/{output_folder}/'\u001b[m\n",
      "\u001b[32m+       # Set up the SageMaker XGBoost Estimator with custom script\u001b[m\n",
      "\u001b[32m+       xgboost_estimator = XGBoost(\u001b[m\n",
      "\u001b[32m+           entry_point='train_xgboost.py',      # Custom script path\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    source_dir='test_AWS',               # Directory where your script is located\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    source_dir='test_AWS/scripts',               # Directory where your script is located\u001b[m\n",
      "\u001b[32m+           role=role,\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    instance_count=1,\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    instance_type='ml.m5.large',\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_count=instance_count,\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_type=instance_type,\u001b[m\n",
      "\u001b[32m+           output_path=output_path,\u001b[m\n",
      "\u001b[32m+           sagemaker_session=session,\u001b[m\n",
      "\u001b[32m+           framework_version=\"1.5-1\",           # Use latest supported version for better compatibility\u001b[m\n",
      "\u001b[32m+           hyperparameters={\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m        'train': 'titanic_train.csv',\u001b[m\n",
      "\u001b[32m+               'max_depth': 5,\u001b[m\n",
      "\u001b[32m+               'eta': 0.1,\u001b[m\n",
      "\u001b[32m+               'subsample': 0.8,\u001b[m\n",
      "\u001b[32m+      \u001b[36m@@ -36,4 +40,4 @@\u001b[m \u001b[mstart = t.time()\u001b[m\n",
      "\u001b[32m+       xgboost_estimator.fit({'train': train_input})\u001b[m\n",
      "\u001b[32m+       end = t.time()\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-print(f\"Training time on SageMaker: {end - start:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/14:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 06471756-048e-488d-a2d0-0684febe10c6\n",
      "\u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[32m+      \u001b[32m+        []\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      #### Hyperparameters\n",
      "\u001b[32m+      \u001b[32m+      >The `hyperparameters` section in this code defines key parameters for the XGBoost model, such as `max_depth`, `eta`, `subsample`, `colsample_bytree`, and `num_round`. These parameters control aspects of the model, like tree depth, learning rate, and data sampling, which influence model performance and training time. \n",
      "\u001b[32m+      \u001b[32m+      > \n",
      "\u001b[32m+      \u001b[32m+      > When running the training job, SageMaker passes these values to `train_xgboost.py` as command-line arguments, making them accessible in the script via `argparse` or similar methods. This setup enables dynamic tuning of model parameters directly from the training configuration, allowing for flexible experimentation without modifying the training script itself.\n",
      "\u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[32m+      \u001b[32m+    id: d1190ce6-3c45-442c-a39e-32a00b1d01a9\n",
      "\u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[32m+      \u001b[32m+        []\n",
      "\u001b[32m+      \u001b[32m+    metadata (unknown keys):\n",
      "\u001b[32m+      \u001b[32m+      jp-MarkdownHeadingCollapsed: True\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      #### Model results\n",
      "\u001b[32m+      \u001b[32m+      > With this code, the training results and model artifacts are saved in a subfolder called `xgboost` in your specified S3 bucket. This folder (`s3://{bucket}/xgboost/`) will be automatically created if it doesn’t already exist, and will contain:\n",
      "\u001b[32m+      \u001b[32m+      > \n",
      "\u001b[32m+      \u001b[32m+      > 1. **Model Artifacts**: The trained model file (often a `.tar.gz` file) that SageMaker saves in the `output_path`.\n",
      "\u001b[32m+      \u001b[32m+      > 2. **Logs and Metrics**: Any metrics and logs related to the training job, stored in the same `xgboost` folder.\n",
      "\u001b[32m+      \u001b[32m+      > \n",
      "\u001b[32m+      \u001b[32m+      > This setup allows for convenient access to both the trained model and related output for later evaluation or deployment.\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/14/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -2,7 +2,7 @@\u001b[m\n",
      "\u001b[32m+       To evaluate the model on a test set after training, we’ll go through these steps:\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       1. **Download the trained model from S3**.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-2. **Load and preprocess** the test dataset. We don't actually have ground truth test data available for this challenge (labels are missing). If you find yourself in this situation, you may want to exclude a random sample of the train data as your test.csv. In our example below, we get predictions on the train data, but you'll want to adjust this to use test data for your final assessment of model generalizeability.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m2. **Load and preprocess** the test dataset.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[32m+       3. **Evaluate** the model on the test data.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       Here’s how you can implement this in your SageMaker notebook. The following code will:\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/15/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  26\n",
      "\u001b[32m+      \u001b[32m+  38\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/15/outputs/0/text:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[32m+      \u001b[31m-xgboost/sagemaker-xgboost-2024-10-29-21-48-04-392/output/model.tar.gz\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mxgboost/sagemaker-xgboost-2024-11-01-22-25-59-504/output/model.tar.gz\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/15/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -3,10 +3,6 @@\u001b[m \u001b[mmodel_s3_path = f'{output_folder}/{xgboost_estimator.latest_training_job.name}/o\u001b[m\n",
      "\u001b[32m+       print(model_s3_path)\u001b[m\n",
      "\u001b[32m+       local_model_path = 'model.tar.gz'\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-# Load the test set from s3\u001b[m\n",
      "\u001b[32m+      \u001b[31m-s3.download_file(bucket, 'train.csv', 'train.csv') # only using train because we don't have test data in this example. Pretend this is test data for now.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-test_data = pd.read_csv('train.csv')\u001b[m\n",
      "\u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[32m+       # Download the trained model from S3\u001b[m\n",
      "\u001b[32m+       s3.download_file(bucket, model_s3_path, local_model_path)\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/16:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 1ac08164-4383-4460-8181-ea096451dde2\n",
      "\u001b[32m+      \u001b[32m+    execution_count: 40\n",
      "\u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[32m+      \u001b[32m+        []\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      # Load the test set. We downloaded this earlier from our S3 bucket.\n",
      "\u001b[32m+      \u001b[32m+      test_data = pd.read_csv(test_filename)\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/16/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  27\n",
      "\u001b[32m+      \u001b[32m+  41\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/16/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: execute_result\n",
      "\u001b[32m+      \u001b[32m+    execution_count: 41\n",
      "\u001b[32m+      \u001b[32m+    data:\n",
      "\u001b[32m+      \u001b[32m+      text/html:\n",
      "\u001b[32m+      \u001b[32m+        <div>\n",
      "\u001b[32m+      \u001b[32m+        <style scoped>\n",
      "\u001b[32m+      \u001b[32m+            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[32m+      \u001b[32m+                vertical-align: middle;\n",
      "\u001b[32m+      \u001b[32m+            }\n",
      "\u001b[32m+      \u001b[32m+        \n",
      "\u001b[32m+      \u001b[32m+            .dataframe tbody tr th {\n",
      "\u001b[32m+      \u001b[32m+                vertical-align: top;\n",
      "\u001b[32m+      \u001b[32m+            }\n",
      "\u001b[32m+      \u001b[32m+        \n",
      "\u001b[32m+      \u001b[32m+            .dataframe thead th {\n",
      "\u001b[32m+      \u001b[32m+                text-align: right;\n",
      "\u001b[32m+      \u001b[32m+            }\n",
      "\u001b[32m+      \u001b[32m+        </style>\n",
      "\u001b[32m+      \u001b[32m+        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[32m+      \u001b[32m+          <thead>\n",
      "\u001b[32m+      \u001b[32m+            <tr style=\"text-align: right;\">\n",
      "\u001b[32m+      \u001b[32m+              <th></th>\n",
      "\u001b[32m+      \u001b[32m+              <th>PassengerId</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Survived</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Pclass</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Name</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Sex</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Age</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>SibSp</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Parch</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Ticket</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Fare</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Cabin</th>\n",
      "\u001b[32m+      \u001b[32m+              <th>Embarked</th>\n",
      "\u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[32m+      \u001b[32m+          </thead>\n",
      "\u001b[32m+      \u001b[32m+          <tbody>\n",
      "\u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[32m+      \u001b[32m+              <th>0</th>\n",
      "\u001b[32m+      \u001b[32m+              <td>566</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>Davies, Mr. Alfred J</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>24.0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>2</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>A/4 48871</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>24.1500</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>S</td>\n",
      "\u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[32m+      \u001b[32m+              <th>1</th>\n",
      "\u001b[32m+      \u001b[32m+              <td>161</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>Cribb, Mr. John Hatfield</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>44.0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>371362</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>16.1000</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>S</td>\n",
      "\u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[32m+      \u001b[32m+              <th>2</th>\n",
      "\u001b[32m+      \u001b[32m+              <td>554</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>Leeni, Mr. Fahim (\"Philip Zenni\")</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>22.0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>2620</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>7.2250</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>C</td>\n",
      "\u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[32m+      \u001b[32m+              <th>3</th>\n",
      "\u001b[32m+      \u001b[32m+              <td>861</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>Hansen, Mr. Claus Peter</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>41.0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>2</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>350026</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>14.1083</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>S</td>\n",
      "\u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[32m+      \u001b[32m+              <th>4</th>\n",
      "\u001b[32m+      \u001b[32m+              <td>242</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>Murphy, Miss. Katherine \"Kate\"</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>female</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>367230</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>15.5000</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[32m+              <td>Q</td>\n",
      "\u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[32m+      \u001b[32m+          </tbody>\n",
      "\u001b[32m+      \u001b[32m+        </table>\n",
      "\u001b[32m+      \u001b[32m+        </div>\n",
      "\u001b[32m+      \u001b[32m+      text/plain:\n",
      "\u001b[32m+      \u001b[32m+           PassengerId  Survived  Pclass                               Name     Sex  \\\n",
      "\u001b[32m+      \u001b[32m+        0          566         0       3               Davies, Mr. Alfred J    male   \n",
      "\u001b[32m+      \u001b[32m+        1          161         0       3           Cribb, Mr. John Hatfield    male   \n",
      "\u001b[32m+      \u001b[32m+        2          554         1       3  Leeni, Mr. Fahim (\"Philip Zenni\")    male   \n",
      "\u001b[32m+      \u001b[32m+        3          861         0       3            Hansen, Mr. Claus Peter    male   \n",
      "\u001b[32m+      \u001b[32m+        4          242         1       3     Murphy, Miss. Katherine \"Kate\"  female   \n",
      "\u001b[32m+      \u001b[32m+        \n",
      "\u001b[32m+      \u001b[32m+            Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
      "\u001b[32m+      \u001b[32m+        0  24.0      2      0  A/4 48871  24.1500   NaN        S  \n",
      "\u001b[32m+      \u001b[32m+        1  44.0      0      1     371362  16.1000   NaN        S  \n",
      "\u001b[32m+      \u001b[32m+        2  22.0      0      0       2620   7.2250   NaN        C  \n",
      "\u001b[32m+      \u001b[32m+        3  41.0      2      0     350026  14.1083   NaN        S  \n",
      "\u001b[32m+      \u001b[32m+        4   NaN      1      0     367230  15.5000   NaN        Q  \n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/16/outputs/0:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: execute_result\n",
      "\u001b[32m+      \u001b[31m-    execution_count: 27\n",
      "\u001b[32m+      \u001b[31m-    data:\n",
      "\u001b[32m+      \u001b[31m-      text/html:\n",
      "\u001b[32m+      \u001b[31m-        <div>\n",
      "\u001b[32m+      \u001b[31m-        <style scoped>\n",
      "\u001b[32m+      \u001b[31m-            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[32m+      \u001b[31m-                vertical-align: middle;\n",
      "\u001b[32m+      \u001b[31m-            }\n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-            .dataframe tbody tr th {\n",
      "\u001b[32m+      \u001b[31m-                vertical-align: top;\n",
      "\u001b[32m+      \u001b[31m-            }\n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-            .dataframe thead th {\n",
      "\u001b[32m+      \u001b[31m-                text-align: right;\n",
      "\u001b[32m+      \u001b[31m-            }\n",
      "\u001b[32m+      \u001b[31m-        </style>\n",
      "\u001b[32m+      \u001b[31m-        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[32m+      \u001b[31m-          <thead>\n",
      "\u001b[32m+      \u001b[31m-            <tr style=\"text-align: right;\">\n",
      "\u001b[32m+      \u001b[31m-              <th></th>\n",
      "\u001b[32m+      \u001b[31m-              <th>PassengerId</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Survived</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Pclass</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Name</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Sex</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Age</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>SibSp</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Parch</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Ticket</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Fare</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Cabin</th>\n",
      "\u001b[32m+      \u001b[31m-              <th>Embarked</th>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-          </thead>\n",
      "\u001b[32m+      \u001b[31m-          <tbody>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>0</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Braund, Mr. Owen Harris</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>22.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>A/5 21171</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>7.2500</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>1</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>2</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>38.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>PC 17599</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>71.2833</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>C85</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>C</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>2</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Heikkinen, Miss. Laina</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>26.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>STON/O2. 3101282</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>7.9250</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>3</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>4</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>35.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>113803</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>53.1000</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>C123</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[32m+      \u001b[31m-              <th>4</th>\n",
      "\u001b[32m+      \u001b[31m-              <td>5</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>Allen, Mr. William Henry</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>35.0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>373450</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>8.0500</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[32m+      \u001b[31m-          </tbody>\n",
      "\u001b[32m+      \u001b[31m-        </table>\n",
      "\u001b[32m+      \u001b[31m-        </div>\n",
      "\u001b[32m+      \u001b[31m-      text/plain:\n",
      "\u001b[32m+      \u001b[31m-           PassengerId  Survived  Pclass  \\\n",
      "\u001b[32m+      \u001b[31m-        0            1         0       3   \n",
      "\u001b[32m+      \u001b[31m-        1            2         1       1   \n",
      "\u001b[32m+      \u001b[31m-        2            3         1       3   \n",
      "\u001b[32m+      \u001b[31m-        3            4         1       1   \n",
      "\u001b[32m+      \u001b[31m-        4            5         0       3   \n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-                                                        Name     Sex   Age  SibSp  \\\n",
      "\u001b[32m+      \u001b[31m-        0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "\u001b[32m+      \u001b[31m-        1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "\u001b[32m+      \u001b[31m-        2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "\u001b[32m+      \u001b[31m-        3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "\u001b[32m+      \u001b[31m-        4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\u001b[32m+      \u001b[31m-        \n",
      "\u001b[32m+      \u001b[31m-           Parch            Ticket     Fare Cabin Embarked  \n",
      "\u001b[32m+      \u001b[31m-        0      0         A/5 21171   7.2500   NaN        S  \n",
      "\u001b[32m+      \u001b[31m-        1      0          PC 17599  71.2833   C85        C  \n",
      "\u001b[32m+      \u001b[31m-        2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "\u001b[32m+      \u001b[31m-        3      0            113803  53.1000  C123        S  \n",
      "\u001b[32m+      \u001b[31m-        4      0            373450   8.0500   NaN        S  \n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/17/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  28\n",
      "\u001b[32m+      \u001b[32m+  43\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/17/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[32m+       # Preprocess the test set to match the training setup\u001b[m\n",
      "\u001b[32m+      \u001b[31m-from test_AWS.train_xgboost import preprocess_data\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mfrom test_AWS.scripts.train_xgboost import preprocess_data\u001b[m\n",
      "\u001b[32m+       X_test, y_test = preprocess_data(test_data)\u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/18/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  29\n",
      "\u001b[32m+      \u001b[32m+  44\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/18/outputs/0/text:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[32m+      \u001b[31m-Test Set Accuracy: 0.9125\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mTest Set Accuracy: 0.7933\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/18/outputs/1:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[32m+      \u001b[32m+    name: stderr\n",
      "\u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[32m+      \u001b[32m+      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "\u001b[32m+      \u001b[32m+      configuration generated by an older version of XGBoost, please export the model by calling\n",
      "\u001b[32m+      \u001b[32m+      `Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+          https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      for more details about differences between saving model and serializing.\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+        warnings.warn(smsg, UserWarning)\n",
      "\u001b[32m+      \u001b[32m+      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "\u001b[32m+      \u001b[32m+        warnings.warn(smsg, UserWarning)\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/18/outputs/1:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stderr\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:09:15] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "\u001b[32m+      \u001b[31m-        warnings.warn(smsg, UserWarning)\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/20:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 90e53219-1e9f-4dd4-a8a9-150ac540876d\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      #### Setting up the data path\n",
      "\u001b[32m+      \u001b[32m+      In this approach, using `TrainingInput` directly with SageMaker’s built-in XGBoost container contrasts with our previous method, where we specified a custom script with argument inputs (specified in hyperparameters) for data paths and settings. With `TrainingInput`, data paths and formats are managed as structured inputs (`{'train': train_input}`) rather than passed as arguments in a script. This setup simplifies and standardizes data handling in SageMaker’s built-in algorithms, keeping the data configuration separate from hyperparameters.\n",
      "\u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 6105a27f-099e-40f0-8039-4a983ea412cb\n",
      "\u001b[32m+      \u001b[32m+    execution_count: 48\n",
      "\u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[32m+      \u001b[32m+        []\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      train_s3_path\n",
      "\u001b[32m+      \u001b[32m+    outputs:\n",
      "\u001b[32m+      \u001b[32m+      output 0:\n",
      "\u001b[32m+      \u001b[32m+        output_type: execute_result\n",
      "\u001b[32m+      \u001b[32m+        execution_count: 48\n",
      "\u001b[32m+      \u001b[32m+        data:\n",
      "\u001b[32m+      \u001b[32m+          text/plain: 's3://titanic-dataset-test/data/titanic_train.csv'\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/20/execution_count:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  10\n",
      "\u001b[32m+      \u001b[32m+  49\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/20/outputs/0/text:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,2 +1,2 @@\u001b[m\n",
      "\u001b[32m+       INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-19-54-20-817\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mINFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-48-56-105\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/20/outputs/1:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:48:57 Starting - Starting the training job...\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:49:11 Starting - Preparing the instances for training...\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:49:42 Downloading - Downloading input data...\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:50:22 Downloading - Downloading the training image......\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:51:29 Training - Training image download completed. Training in progress.\n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:51:29 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[32m+        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:23.778 ip-10-2-247-41.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:23.803 ip-10-2-247-41.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Single node training.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Train matrix has 713 rows and 11 columns\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.213 ip-10-2-247-41.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.214 ip-10-2-247-41.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[0]#011train-rmse:474.71030\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.227 ip-10-2-247-41.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.229 ip-10-2-247-41.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[1]#011train-rmse:441.03842\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[2]#011train-rmse:411.78134\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[3]#011train-rmse:385.35440\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[4]#011train-rmse:362.34192\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[5]#011train-rmse:342.72199\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[6]#011train-rmse:325.37424\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[7]#011train-rmse:310.20413\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[8]#011train-rmse:297.79462\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[9]#011train-rmse:287.85199\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[10]#011train-rmse:277.92941\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[11]#011train-rmse:270.85162\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[12]#011train-rmse:263.09851\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[13]#011train-rmse:257.25269\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[14]#011train-rmse:251.85989\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[15]#011train-rmse:247.19409\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[16]#011train-rmse:243.73045\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[17]#011train-rmse:240.81642\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[18]#011train-rmse:238.41530\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[19]#011train-rmse:235.56351\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[20]#011train-rmse:233.57898\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[21]#011train-rmse:231.39540\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[22]#011train-rmse:228.63503\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[23]#011train-rmse:226.69484\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[24]#011train-rmse:225.35779\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[25]#011train-rmse:223.92523\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[26]#011train-rmse:222.10831\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[27]#011train-rmse:219.23029\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[28]#011train-rmse:218.87340\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[29]#011train-rmse:216.75085\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[30]#011train-rmse:215.76749\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[31]#011train-rmse:214.97679\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[32]#011train-rmse:213.81511\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[33]#011train-rmse:212.42398\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[34]#011train-rmse:211.10745\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[35]#011train-rmse:209.56615\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[36]#011train-rmse:208.38251\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[37]#011train-rmse:207.96460\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[38]#011train-rmse:206.41853\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[39]#011train-rmse:205.13840\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[40]#011train-rmse:204.59671\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[41]#011train-rmse:203.43626\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[42]#011train-rmse:202.23776\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[43]#011train-rmse:201.98227\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[44]#011train-rmse:201.53015\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[45]#011train-rmse:200.83151\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[46]#011train-rmse:199.75769\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[47]#011train-rmse:197.73955\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[48]#011train-rmse:196.67972\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[49]#011train-rmse:195.99304\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[50]#011train-rmse:194.60979\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[51]#011train-rmse:193.87764\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[52]#011train-rmse:193.04419\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[53]#011train-rmse:191.84062\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[54]#011train-rmse:191.63332\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[55]#011train-rmse:191.06137\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[56]#011train-rmse:190.63503\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[57]#011train-rmse:190.23791\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[58]#011train-rmse:190.01700\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[59]#011train-rmse:189.62627\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[60]#011train-rmse:188.78932\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[61]#011train-rmse:187.87903\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[62]#011train-rmse:187.33061\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[63]#011train-rmse:186.93269\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[64]#011train-rmse:186.04112\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[65]#011train-rmse:185.29774\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[66]#011train-rmse:184.67114\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[67]#011train-rmse:183.74358\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[68]#011train-rmse:183.30225\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[69]#011train-rmse:182.09914\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[70]#011train-rmse:181.83897\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[71]#011train-rmse:181.03862\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[72]#011train-rmse:180.78651\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[73]#011train-rmse:179.64867\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[74]#011train-rmse:178.82935\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[75]#011train-rmse:178.21071\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[76]#011train-rmse:177.54585\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[77]#011train-rmse:177.00539\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[78]#011train-rmse:176.26054\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[79]#011train-rmse:175.64746\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[80]#011train-rmse:174.62911\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[81]#011train-rmse:174.01623\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[82]#011train-rmse:173.50301\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[83]#011train-rmse:172.43010\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[84]#011train-rmse:171.95624\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[85]#011train-rmse:171.48639\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[86]#011train-rmse:171.19154\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[87]#011train-rmse:169.97925\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[88]#011train-rmse:169.45494\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[89]#011train-rmse:168.90468\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[90]#011train-rmse:168.16402\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[91]#011train-rmse:167.30739\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[92]#011train-rmse:166.85228\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[93]#011train-rmse:165.98686\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[94]#011train-rmse:165.70697\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[95]#011train-rmse:165.43739\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[96]#011train-rmse:164.83107\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[97]#011train-rmse:164.22020\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[98]#011train-rmse:163.90085\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \u001b[34m[99]#011train-rmse:163.45399\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      \n",
      "\u001b[32m+      \u001b[32m+      2024-11-01 22:51:43 Completed - Training job completed\n",
      "\u001b[32m+      \u001b[32m+      Training seconds: 120\n",
      "\u001b[32m+      \u001b[32m+      Billable seconds: 120\n",
      "\u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[32m+      \u001b[32m+    output_type: error\n",
      "\u001b[32m+      \u001b[32m+    ename: NameError\n",
      "\u001b[32m+      \u001b[32m+    evalue: name 'instance_type' is not defined\n",
      "\u001b[32m+      \u001b[32m+    traceback:\n",
      "\u001b[32m+      \u001b[32m+      item[0]: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+      item[1]: \u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[32m+      \u001b[32m+      item[2]:\n",
      "\u001b[32m+      \u001b[32m+        Cell \u001b[0;32mIn[49], line 28\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+        \u001b[1;32m     25\u001b[0m xgboost_estimator_builtin\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_input})\n",
      "\u001b[32m+      \u001b[32m+        \u001b[1;32m     26\u001b[0m end \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[32m+      \u001b[32m+        \u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime for training on SageMaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, instance_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43minstance_type\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, instance_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[32m+      \u001b[32m+      item[3]: \u001b[0;31mNameError\u001b[0m: name 'instance_type' is not defined\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/20/outputs/1:\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:54:24 Starting - Starting the training job...\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:54:38 Starting - Preparing the instances for training...\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:55:10 Downloading - Downloading input data...\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:55:55 Downloading - Downloading the training image......\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:56:51 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[32m+      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.068 ip-10-0-106-124.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.103 ip-10-0-106-124.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Single node training.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.643 ip-10-0-106-124.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.644 ip-10-0-106-124.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.645 ip-10-0-106-124.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.645 ip-10-0-106-124.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[0]#011train-rmse:475.71411\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.661 ip-10-0-106-124.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.663 ip-10-0-106-124.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[1]#011train-rmse:441.49982\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[2]#011train-rmse:411.72672\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[3]#011train-rmse:385.56036\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[4]#011train-rmse:363.56360\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[5]#011train-rmse:344.49588\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[6]#011train-rmse:327.67941\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[7]#011train-rmse:312.88376\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[8]#011train-rmse:299.41186\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[9]#011train-rmse:288.74734\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[10]#011train-rmse:280.12149\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[11]#011train-rmse:273.00226\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[12]#011train-rmse:266.27246\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[13]#011train-rmse:259.31256\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[14]#011train-rmse:253.75191\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[15]#011train-rmse:249.62512\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[16]#011train-rmse:245.67200\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[17]#011train-rmse:242.34294\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[18]#011train-rmse:238.86392\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[19]#011train-rmse:235.89893\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[20]#011train-rmse:233.08173\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[21]#011train-rmse:231.52962\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[22]#011train-rmse:229.66519\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[23]#011train-rmse:227.86470\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[24]#011train-rmse:226.72954\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[25]#011train-rmse:225.20438\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[26]#011train-rmse:223.76180\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[27]#011train-rmse:221.96107\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[28]#011train-rmse:221.09845\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[29]#011train-rmse:219.99710\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[30]#011train-rmse:219.37936\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[31]#011train-rmse:218.05364\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[32]#011train-rmse:217.43600\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[33]#011train-rmse:216.73910\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[34]#011train-rmse:216.43459\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[35]#011train-rmse:215.56277\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[36]#011train-rmse:214.80632\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[37]#011train-rmse:213.70375\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[38]#011train-rmse:213.17102\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[39]#011train-rmse:212.88145\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[40]#011train-rmse:211.96532\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[41]#011train-rmse:211.32878\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[42]#011train-rmse:210.17601\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[43]#011train-rmse:209.69156\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[44]#011train-rmse:208.88245\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[45]#011train-rmse:207.83882\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[46]#011train-rmse:206.71755\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[47]#011train-rmse:205.46107\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[48]#011train-rmse:204.46623\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[49]#011train-rmse:203.54263\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[50]#011train-rmse:202.88849\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[51]#011train-rmse:202.33600\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[52]#011train-rmse:201.32494\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[53]#011train-rmse:200.73129\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[54]#011train-rmse:200.46822\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[55]#011train-rmse:199.81789\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[56]#011train-rmse:199.07161\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[57]#011train-rmse:198.43318\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[58]#011train-rmse:198.14304\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[59]#011train-rmse:197.53601\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[60]#011train-rmse:197.10297\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[61]#011train-rmse:196.49066\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[62]#011train-rmse:196.28507\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[63]#011train-rmse:195.67941\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[64]#011train-rmse:195.51599\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[65]#011train-rmse:194.97147\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[66]#011train-rmse:193.90741\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[67]#011train-rmse:193.52390\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[68]#011train-rmse:193.01285\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[69]#011train-rmse:192.34790\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[70]#011train-rmse:191.98561\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[71]#011train-rmse:191.39389\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[72]#011train-rmse:190.95151\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[73]#011train-rmse:190.21582\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[74]#011train-rmse:189.08704\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[75]#011train-rmse:188.47955\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[76]#011train-rmse:188.12349\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[77]#011train-rmse:187.77058\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[78]#011train-rmse:187.10945\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[79]#011train-rmse:186.61465\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[80]#011train-rmse:185.80434\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[81]#011train-rmse:184.99844\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[82]#011train-rmse:184.62537\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[83]#011train-rmse:184.16344\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[84]#011train-rmse:183.58179\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[85]#011train-rmse:183.19162\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[86]#011train-rmse:182.80438\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[87]#011train-rmse:182.35306\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[88]#011train-rmse:181.88933\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[89]#011train-rmse:181.32742\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[90]#011train-rmse:180.80669\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[91]#011train-rmse:180.49007\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[92]#011train-rmse:179.89072\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[93]#011train-rmse:179.37184\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[94]#011train-rmse:179.06938\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[95]#011train-rmse:178.65883\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[96]#011train-rmse:177.99016\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[97]#011train-rmse:177.48030\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[98]#011train-rmse:177.15761\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \u001b[34m[99]#011train-rmse:176.45931\u001b[0m\n",
      "\u001b[32m+      \u001b[31m-      \n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:57:20 Uploading - Uploading generated training model\n",
      "\u001b[32m+      \u001b[31m-      2024-10-29 19:57:20 Completed - Training job completed\n",
      "\u001b[32m+      \u001b[31m-      Training seconds: 130\n",
      "\u001b[32m+      \u001b[31m-      Billable seconds: 130\n",
      "\u001b[32m+      \u001b[31m-      Training time on SageMaker with built-in XGBoost image: 197.50 seconds\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/20/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,11 +1,15 @@\u001b[m\n",
      "\u001b[32m+       from sagemaker.estimator import Estimator # when using images, we use the general Estimator class\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define instance type/count we'll use for training\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+       # Use Estimator directly for built-in container without specifying entry_point\u001b[m\n",
      "\u001b[32m+       xgboost_estimator_builtin = Estimator(\u001b[m\n",
      "\u001b[32m+           image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\"),\u001b[m\n",
      "\u001b[32m+           role=role,\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    instance_count=1,\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    instance_type=\"ml.m5.large\",\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_count=instance_count,\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_type=instance_type,\u001b[m\n",
      "\u001b[32m+           output_path=output_path,\u001b[m\n",
      "\u001b[32m+           sagemaker_session=session,\u001b[m\n",
      "\u001b[32m+           hyperparameters={\u001b[m\n",
      "\u001b[32m+      \u001b[36m@@ -25,4 +29,4 @@\u001b[m \u001b[mstart = t.time()\u001b[m\n",
      "\u001b[32m+       xgboost_estimator_builtin.fit({'train': train_input})\u001b[m\n",
      "\u001b[32m+       end = t.time()\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-print(f\"Training time on SageMaker with built-in XGBoost image: {end - start:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/22/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -20,11 +20,24 @@\u001b[m \u001b[mUpgrading a single instance works well if:\u001b[m\n",
      "\u001b[32m+       Upgrading a single instance is typically the most efficient option in terms of both cost and setup complexity. It avoids the communication overhead associated with multi-instance setups (discussed below) and is well-suited for most small to medium-sized datasets.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       ### Option 2: Use Multiple Instances for Distributed Training\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIf upgrading a single instance doesn’t sufficiently reduce training time, distributed training across multiple instances may be a viable alternative, particularly for larger datasets and complex models. SageMaker supports two primary distributed training techniques: **data parallelism** and **model parallelism**.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-If upgrading a single instance doesn’t sufficiently reduce training time, distributed training across multiple instances may be a viable alternative, particularly for larger datasets and complex models. In SageMaker, setting `instance_count > 1` enables data parallelism, where data is split across instances for simultaneous processing.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m#### Understanding Data Parallelism vs. Model Parallelism\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Data Parallelism**: This approach splits the dataset across multiple instances, allowing each instance to process a subset of the data independently. After each batch, gradients are synchronized across instances to ensure consistent updates to the model. Data parallelism is effective when the model itself fits within an instance’s memory, but the data size or desired training speed requires faster processing through multiple instances.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Model Parallelism**: Model parallelism divides the model itself across multiple instances, making it ideal for very large models (e.g., deep learning models in NLP or image processing) that cannot fit in memory on a single instance. Each instance processes a segment of the model, and results are combined during training. This approach is suitable for memory-intensive models that exceed the capacity of a single instance.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m#### How SageMaker Chooses Between Data and Model Parallelism\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIn SageMaker, the choice between data and model parallelism is not entirely automatic. Here’s how it typically works:\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Data Parallelism (Automatic)**: When you set `instance_count > 1`, SageMaker will automatically apply data parallelism. This splits the dataset across instances, allowing each instance to process a subset independently and synchronize gradients after each batch. Data parallelism works well when the model can fit in the memory of a single instance, but the data size or processing speed needs enhancement with multiple instances.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Model Parallelism (Manual Setup)**: To enable model parallelism, you need to configure it explicitly using the **SageMaker Model Parallel Library**, suitable for deep learning models in frameworks like PyTorch or TensorFlow. Model parallelism splits the model itself across multiple instances, which is useful for memory-intensive models that exceed the capacity of a single instance. Configuring model parallelism requires setting up a distribution strategy in SageMaker’s Python SDK.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Hybrid Parallelism (Manual Setup)**: For extremely large datasets and models, SageMaker can support both data and model parallelism together, but this setup requires manual configuration. Hybrid parallelism is beneficial for workloads that are both data- and memory-intensive, where both the model and the data need distributed processing.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-- **Data Parallelism**: This approach divides the dataset across instances, each instance processing a subset of the data independently. The gradients are then synchronized across instances, which helps reduce training time without requiring the dataset to fit entirely in memory on a single instance.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-- **Model Parallelism**: Useful for extremely large models that don’t fit in memory on a single instance, model parallelism divides the model itself across instances. This is particularly effective for memory-intensive models, such as those in natural language processing and large image processing networks.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       **When to Use Distributed Training with Multiple Instances**  \u001b[m\n",
      "\u001b[32m+       Consider multiple instances if:\u001b[m\n",
      "\u001b[32m+      \u001b[36m@@ -58,10 +71,10 @@\u001b[m \u001b[mLet’s break down some key points for deciding between **1 instance vs. multipl\u001b[m\n",
      "\u001b[32m+            - **10 instances (parallel):** `(T / k) * (10 * $C)`, where `k` is the speedup factor (<10 due to overhead).\u001b[m\n",
      "\u001b[32m+          - If the speedup is only about 5x instead of 10x due to communication overhead, then the cost difference may be minimal, with a slight edge to a single instance on total cost but at a higher wall-clock time.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m----\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[31m-In summary:\u001b[m\n",
      "\u001b[32m+      \u001b[31m-- **Start by upgrading to a more powerful instance (Option 1)** for datasets up to 10 GB and moderately complex models. A single, more powerful, instance is usually more cost-effective for smaller workloads and where time isn’t critical. Running initial tests with a single instance can also provide a benchmark. You can then experiment with small increases in instance count to find a balance between cost and time savings, particularly considering communication overheads that affect parallel efficiency.\u001b[m\n",
      "\u001b[32m+      \u001b[31m-- **Consider distributed training across multiple instances (Option 2)** only when dataset size, model complexity, or training time demand it.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> In summary:\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> - **Start by upgrading to a more powerful instance (Option 1)** for datasets up to 10 GB and moderately complex models. A single, more powerful, instance is usually more cost-effective for smaller workloads and where time isn’t critical. Running initial tests with a single instance can also provide a benchmark. You can then experiment with small increases in instance count to find a balance between cost and time savings, particularly considering communication overheads that affect parallel efficiency.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> - **Consider distributed training across multiple instances (Option 2)** only when dataset size, model complexity, or training time demand it.\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       ---\u001b[m\n",
      "\u001b[32m+      \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/23/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,5 +1,10 @@\u001b[m\n",
      "\u001b[32m+       ## XGBoost's Distributed Training Mechanism\u001b[m\n",
      "\u001b[32m+      \u001b[31m-In the event that option 2 explained above really is better for your use-case (e.g., you have a very large dataset or model that takes a while to train even with high performance instances), the next example will demo setting this up. Before we do, though, we should ask what distributed computing really means for our specific model/setup.XGBoost’s distributed training relies on a data-parallel approach that divides the dataset across multiple instances (or workers), enabling each instance to work on a portion of the data independently. This strategy enhances efficiency, especially for large datasets and computationally intensive tasks. Here’s how distributed training in XGBoost works, particularly in the SageMaker environment:\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIn the event that option 2 explained above really is better for your use-case (e.g., you have a very large dataset or model that takes a while to train even with high performance instances), the next example will demo setting this up. Before we do, though, we should ask what distributed computing really means for our specific model/setup. XGBoost’s distributed training relies on a data-parallel approach that divides the dataset across multiple instances (or workers), enabling each instance to work on a portion of the data independently. This strategy enhances efficiency, especially for large datasets and computationally intensive tasks.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> **What about a model parallelism approach?** Unlike deep learning models with vast neural network layers, XGBoost’s decision trees are usually small enough to fit in memory on a single instance, even when the dataset is large. Thus, model parallelism is rarely necessary.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mXGBoost does not inherently support model parallelism out of the box in SageMaker because the model architecture doesn’t typically exceed memory limits, unlike massive language or image models. Although model parallelism can be theoretically applied (e.g., splitting large tree structures across instances), it's generally not supported natively in SageMaker for XGBoost, as it would require a custom distribution framework to split the model itself.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mHere’s how distributed training in XGBoost works, particularly in the SageMaker environment:\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       ### Key Steps in Distributed Training with XGBoost\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/24/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -1,12 +1,16 @@\u001b[m\n",
      "\u001b[32m+       from sagemaker.inputs import TrainingInput\u001b[m\n",
      "\u001b[32m+       from sagemaker.estimator import Estimator\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define instance type/count we'll use for training\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+       # Define the XGBoost estimator for distributed training\u001b[m\n",
      "\u001b[32m+       xgboost_estimator = Estimator(\u001b[m\n",
      "\u001b[32m+           image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\"),\u001b[m\n",
      "\u001b[32m+           role=role,\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    instance_count=1,  # Start with 1 instance for baseline\u001b[m\n",
      "\u001b[32m+      \u001b[31m-    instance_type=\"ml.m5.large\",\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_count=instance_count,  # Start with 1 instance for baseline\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_type=instance_type,\u001b[m\n",
      "\u001b[32m+           output_path=output_path,\u001b[m\n",
      "\u001b[32m+           sagemaker_session=session,\u001b[m\n",
      "\u001b[32m+       )\u001b[m\n",
      "\u001b[32m+      \u001b[36m@@ -21,15 +25,16 @@\u001b[m \u001b[mxgboost_estimator.set_hyperparameters(\u001b[m\n",
      "\u001b[32m+       )\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Specify input data from S3\u001b[m\n",
      "\u001b[32m+      \u001b[31m-train_input = TrainingInput(f\"s3://{bucket}/train.csv\", content_type=\"csv\")\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtrain_input = TrainingInput(train_s3_path, content_type=\"csv\")\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Run with 1 instance\u001b[m\n",
      "\u001b[32m+       start_time = t.time()\u001b[m\n",
      "\u001b[32m+       xgboost_estimator.fit({\"train\": train_input})\u001b[m\n",
      "\u001b[32m+      \u001b[31m-print(f\"Training time with 1 instance: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {xgboost_estimator.instance_count}\")\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Now run with 2 instances to observe speedup\u001b[m\n",
      "\u001b[32m+       xgboost_estimator.instance_count = 2\u001b[m\n",
      "\u001b[32m+       start_time = t.time()\u001b[m\n",
      "\u001b[32m+       xgboost_estimator.fit({\"train\": train_input})\u001b[m\n",
      "\u001b[32m+      \u001b[31m-print(f\"Training time with 2 instances: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {xgboost_estimator.instance_count}\")\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/26:\u001b[0m\n",
      "\u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[32m+      \u001b[32m+    id: d2e722d7-93d0-456d-8a5b-3c3f9e89c5b7\n",
      "\u001b[32m+      \u001b[32m+    execution_count: 51\n",
      "\u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[32m+      \u001b[32m+        []\n",
      "\u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[32m+      \u001b[32m+      print('yay, code this far up works')\n",
      "\u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[32m+      \u001b[32m+    id: 802be838-91b5-43bb-b7e1-c3a70250d22d\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/27/source:\u001b[0m\n",
      "\u001b[32m+      \u001b[36m@@ -4,7 +4,7 @@\u001b[m \u001b[mfrom sklearn.preprocessing import StandardScaler, LabelEncoder\u001b[m\n",
      "\u001b[32m+       import numpy as np\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Load and preprocess the Titanic dataset\u001b[m\n",
      "\u001b[32m+      \u001b[31m-df = pd.read_csv('train.csv')\u001b[m\n",
      "\u001b[32m+      \u001b[32m+\u001b[m\u001b[32mdf = pd.read_csv(train_filename)\u001b[m\n",
      "\u001b[32m+       \u001b[m\n",
      "\u001b[32m+       # Encode categorical variables and normalize numerical ones\u001b[m\n",
      "\u001b[32m+       df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\u001b[m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      \u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/4/execution_count:\u001b[0m\n",
      "\u001b[31m-  None\n",
      "\u001b[32m+  5\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## added /cells/4/metadata/tags:\u001b[0m\n",
      "\u001b[32m+  []\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/4/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      [main cc909ef] Updates from Jupyter notebooks\n",
      "\u001b[32m+       4 files changed, 525 insertions(+), 516 deletions(-)\n",
      "\u001b[32m+       create mode 100644 scripts/__pycache__/train_xgboost.cpython-310.pyc\n",
      "\u001b[32m+      From https://github.com/UW-Madison-DataScience/test_AWS\n",
      "\u001b[32m+       * branch            main       -> FETCH_HEAD\n",
      "\u001b[32m+         22d83d0..8102f2c  main       -> origin/main\n",
      "\u001b[32m+      Already up to date.\n",
      "\u001b[32m+      remote: Support for password authentication was removed on August 13, 2021.\n",
      "\u001b[32m+      remote: Please see https://docs.github.com/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n",
      "\u001b[32m+      fatal: Authentication failed for 'https://github.com/UW-Madison-DataScience/test_AWS.git/'\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80ca60f-5e62-45e3-bfbb-9b62712688e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main d6b124c] Updates from Jupyter notebooks\n",
      " 2 files changed, 1810 insertions(+), 864 deletions(-)\n",
      "From https://github.com/UW-Madison-DataScience/test_AWS\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Enumerating objects: 18, done.\n",
      "Counting objects: 100% (18/18), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (12/12), done.\n",
      "Writing objects: 100% (12/12), 24.20 KiB | 1.51 MiB/s, done.\n",
      "Total 12 (delta 7), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (7/7), completed with 4 local objects.\u001b[K\n",
      "To https://github.com/UW-Madison-DataScience/test_AWS.git\n",
      "   8102f2c..d6b124c  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add . # you may also add files one at a time, for further specificity over the associated commit message\n",
    "!git commit -m \"Updates from Jupyter notebooks\" # in general, your commit message should be more specific!\n",
    "\n",
    "!git config pull.rebase false # Combines the remote changes into your local branch as a merge commit.\n",
    "!git pull origin main\n",
    "\n",
    "!git push https://{username}:{token}@{github_url} main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266fce9-b4b4-4134-88d6-7fdcd736a32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f6f61-28d3-4be4-9e93-f5bedcbd9780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd0667-9ebd-418f-bafc-c8ff1126425a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbaca0-b333-4fc0-b185-d6b3a123e9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f88fc0-4dc5-43b8-aa1f-1faaed6ecd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34be5f6-f87d-43aa-8dc0-99030a92caf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f393cf-e481-4dfd-acbf-3862b1488e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bf6dd-d191-404d-9a27-35101349d400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
